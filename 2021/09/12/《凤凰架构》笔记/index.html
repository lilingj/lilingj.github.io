<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="第一部分 演进中的架构第一章 服务架构演进史 原始分布式时代：寻找使用多台计算机共同协作来支撑同一套软件系统的可行方案。设计向性能做出的妥协，令DCE“尽量简单透明”的努力几乎全部付诸东流，无论是从编码、设计、部署还是从运行效率上看，远程都与本地有着天壤之别 单体系统时代：巨石系统，Monolithic Application。 单体除了难以阻断错误传播、不便于动态更新程序以外，还面临难以技术异构">
<meta property="og:type" content="article">
<meta property="og:title" content="《凤凰架构》笔记">
<meta property="og:url" content="http://example.com/2021/09/12/%E3%80%8A%E5%87%A4%E5%87%B0%E6%9E%B6%E6%9E%84%E3%80%8B%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="换菜刀换脸盆">
<meta property="og:description" content="第一部分 演进中的架构第一章 服务架构演进史 原始分布式时代：寻找使用多台计算机共同协作来支撑同一套软件系统的可行方案。设计向性能做出的妥协，令DCE“尽量简单透明”的努力几乎全部付诸东流，无论是从编码、设计、部署还是从运行效率上看，远程都与本地有着天壤之别 单体系统时代：巨石系统，Monolithic Application。 单体除了难以阻断错误传播、不便于动态更新程序以外，还面临难以技术异构">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2021-09-12T15:00:00.000Z">
<meta property="article:modified_time" content="2021-09-12T09:13:31.698Z">
<meta property="article:author" content="lilingj">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/2021/09/12/%E3%80%8A%E5%87%A4%E5%87%B0%E6%9E%B6%E6%9E%84%E3%80%8B%E7%AC%94%E8%AE%B0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>《凤凰架构》笔记 | 换菜刀换脸盆</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">换菜刀换脸盆</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/09/12/%E3%80%8A%E5%87%A4%E5%87%B0%E6%9E%B6%E6%9E%84%E3%80%8B%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="lilingj">
      <meta itemprop="description" content="昼短苦夜长，何不秉烛游？">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="换菜刀换脸盆">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          《凤凰架构》笔记
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-09-12 23:00:00 / 修改时间：17:13:31" itemprop="dateCreated datePublished" datetime="2021-09-12T23:00:00+08:00">2021-09-12</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="第一部分-演进中的架构"><a href="#第一部分-演进中的架构" class="headerlink" title="第一部分 演进中的架构"></a>第一部分 演进中的架构</h1><h2 id="第一章-服务架构演进史"><a href="#第一章-服务架构演进史" class="headerlink" title="第一章 服务架构演进史"></a>第一章 服务架构演进史</h2><ul>
<li><strong>原始分布式时代</strong>：寻找使用多台计算机共同协作来支撑同一套软件系统的可行方案。设计向性能做出的妥协，令DCE“尽量简单透明”的努力几乎全部付诸东流，无论是从编码、设计、部署还是从运行效率上看，远程都与本地有着天壤之别</li>
<li><strong>单体系统时代</strong>：巨石系统，Monolithic Application。<ul>
<li>单体除了难以阻断错误传播、不便于动态更新程序以外，还面临难以技术异构的困难。</li>
<li>单体系统靠高质量来保证高可靠性的思路，在小规模软件上还能运作良好，但当系统规模越来越大时，交付一个可靠的单体系统就变得越来越具有挑战性。</li>
<li><strong>为了允许程序出错，获得自治和隔离能力，以及实现可以技术异构等目标，是继性能与算力之后，让程序再次选择分布式的理由。</strong></li>
<li>在新旧世纪之交，人们曾经探索过几种服务拆分方法，将一个大的单体系统拆分为若干个更小的、不运行在同一个进程的独立服务，这些服务拆分方法后来带来了面向服务架构（Service-Oriented Architecture）的一段兴盛期，我们称其为“SOA时代”</li>
</ul>
</li>
<li><strong>SOA时代</strong>：为了对单体系统进行拆分，让每一个子系统都能独立地部署、运行、更行<ul>
<li><strong>三种比较有代表性的架构模式</strong><ul>
<li><strong>烟囱式架构</strong>：Information Silo Architecture，是一种与其他相关信息系统完全没有互操作或者协调工作的设计模式</li>
<li><strong>微内核架构</strong>：Microkernel Architecture，又称为插件式架构（Plug-in Architecture）。将主数据，连同其他可能被各子系统用到的公共服务、数据、资源集中到一块，组成一个被所有业务系统共同依赖的核心，具体的业务系统以插件模块的形式存在，这样也可以提供可扩展的、灵活的、天然隔离的功能特性，即微内核架构。</li>
<li><strong>事件驱动架构</strong>：Event-Driven Architecture。为了能让子系统互相通信，一种可行的方案是在子系统之间建立一套时间队列管道，来自系统外部的消息将以事件的形式发送到管道中，各个子系统可以从管道里获取自己感兴趣、能够处理的时间信息，也可以为事件新增或者修改其中的附加信息，甚至可以自己发布一些新的事件到管道队列中去。如此，每一条消息的处理者都是独立的、高度解耦的，但又能和其他管理者通过事件管道进行交互。</li>
</ul>
</li>
</ul>
</li>
<li><strong>微服务时代</strong>：微服务是一种通过多个小型服务组合来构建单个应用的架构风格，这些服务围绕业务能力而非特定的技术标准来构建。各个服务可以采用不同的编程语言、不同的数据存储，运行在不同的进程之中。服务采用轻量级的通信机制和自动化的部署机制实现通信与运维<ul>
<li>维基百科仍然将微服务定义为SOA的一种变体，但现在来看，这个定义已经颇有些过时了</li>
<li>微服务不是SOA的变体或衍生品，应该明确地与SOA划清界限，不再贴上任何SOA的标签</li>
<li><strong>微服务的九个核心的业务与技术特征</strong><ul>
<li><strong>围绕业务能力构建</strong>：再次强调康威定律的重要性，有怎样结构、规模、能力的团队，就会产生对应结构、规模、能力的产品。当团队、产品磨合稳定之后，团队与产品就会拥有一致的结构</li>
<li><strong>分散治理</strong>：指服务对应的开发团队有直接对服务运行质量负责的责任，也有不受外界干预地掌控服务各个方面地权力，譬如选择与其他服务异构的技术来实现自己的服务。<strong>微服务更加强调的是在确实需要技术异构时，应能够有选择“不统一”的权利</strong></li>
<li><strong>通过服务来实现独立自治的组件</strong>：通过服务（Service）而不是类库（Library）来来构建组件</li>
<li><strong>产品化思维</strong>：避免把软件研发视作要去完成某种功能，而是视作一种持续改进、提升的过程<ul>
<li>团队应该为软件产品的整个生命周期负责，开发者不仅应该知道软件如何开发，还应该知道它如何运作，用户如何反馈，乃至售后支持工作是怎样进行的</li>
<li>在以前的单体架构下，程序的规模决定了无法让全部成员都关注完整的产品，如开发、运维、支持等不同职责的成员只关注自己的工作，<strong>但在微服务下，要求开发团队中每个人都具有产品化思维，关心整个产品的全部方面是具有可行性的</strong></li>
</ul>
</li>
<li><strong>数据去中心化</strong>：微服务明确提倡数据应该按领域分散管理、更新、维护、存储<ul>
<li>中心化的存储天生就更容易避免一致性问题，但是<strong>同一个数据实体在不同服务的视角里，它的抽象形态往往不同</strong>，如果使用中心化存储，所有领域都必须修改映射到同一个实体之中，这很可能使不同服务相互影响而丧失独立性。尽管在分布式中处理好一致性问题也相当困难，很多时候都没办法使用传统的事务处理来保证，但两害相权取其轻，即使有一些必要的代价，但仍是值得的</li>
</ul>
</li>
<li><strong>强终端弱管道</strong>：Smart Endpoit and Dumb Pipe。如果服务需要上面额外通信能力，就应该在服务自己的Endpoint上解决，而不是在通信管道上一揽子处理<ul>
<li>微服务提倡使用类似于Unix过滤器那样简单直接的通信方式，所以RESTful风格的通信在微服务中会是更合适的选择</li>
</ul>
</li>
<li><strong>容错性设计</strong>：Design for Failure。不再虚幻地追求服务永远稳定，而是接受服务总会出错的事实，要求在微服务的设计中，能够有自动的机制对其依赖的服务进行快速故障检测，在持续出错的时候进行隔离，在服务恢复的时候重新联通<ul>
<li><strong>可靠系统完全可能由会出错的服务组成，这是微服务最大的价值所在，也是本书前言中所说的“凤凰架构”的含义</strong></li>
</ul>
</li>
<li><strong>演进式设计</strong>：容错性设计承认服务会出错，演进式设计承认服务会被报废淘汰。<ul>
<li>一个设计良好的服务，应该是能够报废的，而不是期望得到长存永生。<strong>假如系统中出现不可更改、无可替代的服务，这并不能说明这个服务多么优秀、多么重要，反而是一种系统设计上脆弱的表现，微服务所追求的自治、隔离，也是反对这种脆弱性的表现</strong></li>
</ul>
</li>
<li><strong>基础设施自动化</strong>：由于微服务架构下运维对象数量是单体架构运维对象数量的数量级倍，使用微服务的团队更加依赖于基础设施的自动化，人工是很难支撑成百上千乃至上万级别的服务的</li>
</ul>
</li>
</ul>
</li>
<li><strong>后微服务时代（云原生时代）</strong>：虚拟化技术和容器化技术：软件可以只使用键盘命令就拆分出不同的服务，只通过拷贝、启动就能实现伸缩扩容服务，硬件难道就不可以通过键盘命令变出相应的应用服务器、负载均衡器、DNS服务器、网络链路这些设施吗？<ul>
<li>早期的容器只被简单地视为一种可快速启动的服务运行环境，目的是方便程序的分发部署，在这个阶段，正对单个应用进行封装的容器并未真正解决分布式架构问题</li>
<li>当虚拟化的基础设施从单个服务的容器扩展至由多个容器构成的服务集群、通信网络和存储设施时，软件和硬件的界限便已模糊。一旦虚拟化硬件能够跟上软件的灵活性，那些与业务无关的技术性问题便有可能从软件层面剥离，悄无声息地在硬件基础设施之内解决，让软件得以只专注业务，真正围绕业务能力构建团队和产品</li>
<li>从软件层面独立应对分布式架构所带来的各种问题，发展到应用代码与基础设施软、硬一体，合力应对架构问题，这个新的时代现在常被媒体冠以“云原生”这个颇为抽象的名字加以宣传</li>
<li><strong>服务网格</strong>：<ul>
<li>Kubernetes成为容器战争胜利者标志着后微服务时代的开启，但K8s仍然没能完美解决分布式问题。这是因为有一些问题处于应用系统与基础设施的边缘，使得很难完全在基础设施层面中精细化地处理。</li>
<li>因为基础设施是针对整个容器来管理的，粒度相对粗犷，只能到容器层面，对单个远程服务则很难有效管控。类似的，在服务的监控、认证、授权、安全、负载均衡等方面都有可能面临精细化管理的需求</li>
<li>为了解决这类问题，虚拟化的基础设施很快完成了第二次进化，引入今天被称为<strong>服务网格</strong>的<strong>边车代理模式</strong>。在虚拟化场景中的边车指的是由系统自动在服务容器中注入一个通信代理服务器，以类似中间人攻击的方式进行流量劫持，悄然接管应用所有对外通信。这个代理处了实现正常的服务间通信外，还接受来自控制器的指令，根据控制平面中的配置，对数据平面通信的内容进行分析处理，以实现熔断、认证、度量、监控、负载均衡等各种附加功能</li>
<li>通过边车代理模式，便实现了既不需要在应用层面加入额外的处理代码，也提供了几乎不亚于程序代码的精细管理能力</li>
</ul>
</li>
</ul>
</li>
<li><strong>无服务时代</strong>：无服务也是以“简单”为主要卖点的，它只涉及到两块内容：后端设施和函数<ul>
<li><strong>后端设施</strong>：后端设施是指数据库、消息队列、日志、存储等这类用于支撑业务逻辑运行，但本身无业务含义的技术组件，这些后端设施都运行在云中，在无服务中将它们称为“后端即服务（BaaS）”</li>
<li><strong>函数</strong>：函数是指业务逻辑代码，这里函数的概念与粒度都已经很接近于程序编码角度的函数了，其区别是无服务中的函数运行在云端，不必考虑算力问题，也不必考虑容量规划，在无服务中将其称为“函数即服务”（FaaS）</li>
</ul>
</li>
</ul>
<p><strong>康威定律</strong>：”设计系统的架构受制于产生这些设计的组织的沟通结构。“</p>
<p>杂：</p>
<ul>
<li>微服务追求的是更加自由的架构风格，摒弃了几乎所有SOA里可以抛弃的约束和规定，提倡以“实践标准”代替“规范标准”</li>
<li>一个简单服务，并不见得会同时面临分布式中的所有问题，也就没有必要背上SOA那百宝袋般沉重的技术包袱</li>
<li>作为一个普通服务开发者，作为一个螺丝钉式的程序员，微服务架构是友善的。可是，微服务对架构者确是满满的“恶意”，对架构能力的要求已提升到史无前例的程度</li>
<li>技术架构者的第一职责就是决策权衡，有利有弊才需要决策，有取有舍才需要权衡，如果架构者本身的知识面不足以覆盖所需要决策的内容，不清楚其中利弊，恐怕将无可避免地陷入选择困难症地境遇之中</li>
<li>微服务只需要考虑业务逻辑本身，这才是最理想的智能终端解决方案</li>
<li>如果说微服务架构是分布式系统这条路当前所能做到的极致，那无服务架构，也许就是“不分布式”的云端系统这条路的起点。虽然在顺序上笔者将“无服务”安排到了“微服务”和“云原生”时代之后，但它们并没有继承替代关系</li>
<li>将无服务作为技术层面的架构，将微服务视为应用层面的架构，把它们组合起来使用是完全合理可行的。无论是物理机、虚拟机、容器，抑或是无服务云函数，都会是微服务实现方案的候选项之一</li>
<li>我们谈历史，重点不在于考古，而是借历史之名，理解每种架构出现的意义与淘汰的原因，为的是更好的解决今天的现实问题，寻找出未来架构演进的发展道路</li>
</ul>
<h1 id="第二部分-架构师视角"><a href="#第二部分-架构师视角" class="headerlink" title="第二部分 架构师视角"></a>第二部分 架构师视角</h1><h2 id="第二章-访问远程服务"><a href="#第二章-访问远程服务" class="headerlink" title="第二章 访问远程服务"></a>第二章 访问远程服务</h2><p><strong>杂</strong>：</p>
<ul>
<li>远程服务将计算机程序的工作范围从单机扩展至网络，从本地延申到远程是构建分布式系统的首要基础</li>
<li>RPC出现的目的，就是为了让计算机能够与调用本地方法一样去调用远程方法</li>
<li>IPC：进程间通信<ul>
<li><strong>管道</strong>或者<strong>具名管道</strong>：普通管道只用于有亲缘关系的进程间的通信，具名管道允许无亲缘关系的进程间的通信</li>
<li><strong>信号</strong></li>
<li><strong>信号量</strong>：OS提供的一个特殊变量，程序可以在上面进行wait()和notify()操作</li>
<li><strong>消息队列</strong>：Message Queue。POSIX标准中定义了可用于进程间数据量较多的通信的消息队列</li>
<li><strong>共享内存</strong></li>
<li><strong>本地套接字接口</strong>：UNIX Domain Socket，IPC Socket。出于效率考虑，当仅限于本机进程间通信时，套接字接口是被优化过的，不会经过网路协议栈，不需要打包拆包、计算校验和、维护序号和应答等操作，只是简单地将应用层数据从一个进程复制到另一个进程，这种进程间通信方式叫~</li>
</ul>
</li>
<li>《A Critique of The Remote Procedure Call Paradigm》中心观点：把本地调用与远程调用当作同样调用来处理，这是犯了方向性的错误，把系统间的调用透明化，反而会增加程序员工作的复杂度</li>
<li><strong>网络编程中经常忽略的八大问题（8条反话）</strong>：<ul>
<li>网络是可靠的</li>
<li>延迟是不存在的</li>
<li>带宽是无线的</li>
<li>网络是安全的</li>
<li>拓扑结构是一成不变的</li>
<li>总会有一个管理员</li>
<li>不必考虑传输成本</li>
<li>网络都是同质化的</li>
</ul>
</li>
<li>远程服务调用要透明化，就必须为这些问题埋单</li>
<li><strong>得出的主流观点</strong>：RPC应该是一种高层次的或者说语言层次的特征，而不是向IPC那样，是低层次的或者说系统层次的特征</li>
<li><strong>RPC三个基本问题</strong>：<ul>
<li>如何表示数据：数据结构 + 序列化协议</li>
<li>如何传递数据：应用层协议。两个服务交互不只是扔个序列化数据流来表示参数和结果就行，许多在此之外的信息，譬如异常、超时、安全、认证、授权、事务等，都可能产生双方需要交换信息的需求（Wire Protocol）</li>
<li>如何表示方法：<ul>
<li>UUID：每个方法绑定一个UUID</li>
<li>语言无关的接口描述语言</li>
</ul>
</li>
</ul>
</li>
<li>REST和RPC是主流的两种远程调用方式，它们在思想上的差异的核心是抽象的目标不一样，即<strong>面向资源</strong>的编程思想和面向过程的编程思想两者之间的区别。REST只能说是<strong>风格</strong>而不是规范、协议</li>
<li><strong>一套理想的、完全满足REST风格的系统应满足六大原则</strong>：<ul>
<li><strong>客户端与服务端分离</strong>：将用户界面所关注的逻辑和数据存储所关注的逻辑分离开来，有助于提高用户界面跨平台的可移植性（llj：这意思不应该是前后端分离吗？）</li>
<li><strong>无状态</strong>：stateless。无状态是REST的一条核心原则。客户端负责保存所有必要的上下文信息，服务端只依据客户端传递的状态来执行业务处理逻辑，驱动整个应用的状态变迁<ul>
<li>服务端无状态可以在分布式计算中获得非常高的价值回报，但大型系统的上下文状态数量完全可能膨胀到客户端无法承受的程度，<strong>在服务端的内存、会话、数据库或者缓存等地方持有一定的状态称为一种事实上存在，并将长期存在、被广泛使用的主流方案</strong></li>
</ul>
</li>
<li><strong>可缓存</strong>：Cacheability。运作良好的缓存机制可以减少客户端、服务器之间的交互，甚至完全避免，进而进一步提高性能</li>
<li><strong>分层系统</strong>：指客户端一般不需要知道是否直接连接到了最终的服务器，抑或连接到路径上的中间服务器<ul>
<li>中间服务器可以通过负载均衡和共享缓存的机制提高系统的可扩展性，这样也便于缓存、伸缩和安全策略的部署</li>
<li>典型应用：内容分发网络CDN</li>
</ul>
</li>
<li><strong>统一接口</strong>：REST的另一条核心原则。<strong>REST希望开发者面向资源编程</strong>，希望软件系统设计的重点放在抽象系统该有哪些资源，而不是抽象系统该有那些行为（服务）上。这条原则可以类比计算机中对文件管理的操作来理解<ul>
<li>基于网络的软件系统，到底是面向资源编程合适，还是面向服务更合适，这个问题恐怕再很长时间里都不会有定论</li>
<li><strong>面向资源编程的抽象程度通常更高。抽象程度高带来的坏处是距离人类的思维方式往往会更远，而好处是通用程度往往会更好</strong></li>
</ul>
</li>
<li>按需代码：可选原则。指任何按照客户端的请求，将可执行的软件程序从服务端发送到客户端的技术</li>
</ul>
</li>
<li><strong>RMM</strong>：Richardson Maturity Model，Richardson成熟度模型。衡量“服务有多么REST”的模型<ul>
<li>第0级：完全不REST</li>
<li>第1级：开始引入资源的概念</li>
<li>第2级：引入统一接口，映射到HTTP协议的方法上</li>
<li>第3级：超媒体控制</li>
</ul>
</li>
</ul>
<h2 id="第三章-事务处理"><a href="#第三章-事务处理" class="headerlink" title="第三章 事务处理"></a>第三章 事务处理</h2><ul>
<li>事务：事务处理几乎再每个信息系统中都会涉及<ul>
<li>意义：为了保证系统中所有的数据都是符合期望的，且相互关联的数据之间不会产生矛盾，即数据状态的<strong>一致性</strong></li>
<li>特性：ACID，AID是手段，是因，C是目的，是果<ul>
<li>原子性：<strong>Atomic</strong>。同一项业务处理过程中，对数据的操作要么全部成功，要么全部失败</li>
<li>隔离性：<strong>Isolation</strong>。各业务读写的数据相互独立，不会彼此影响</li>
<li>持久性：<strong>Durability</strong>。应该保证所有成功被提交的数据修改都能够正确地被持久化，不丢失数据</li>
<li>一致性：<strong>Consistency</strong>。~<ul>
<li><strong>内部一致性</strong>：单个数据源</li>
<li><strong>外部一致性</strong>：涉及多个数据源的事务间的一致性</li>
</ul>
</li>
</ul>
</li>
<li>外部一致性通常很难用AID来解决。为此我们要转变观念，将一致性从“是或否”的二元属性转变为可以按不同强度分开讨论的多元属性，在确保代价可承受的前提下获得强度尽可能高的一致性保障，也正因为如此，事务处理才从要给具体操作上的“编程问题”上升成一个需要全局权衡的“架构问题”</li>
</ul>
</li>
<li><strong>实现事务的原子性和持久性的方案</strong><ul>
<li><strong>提交日志</strong>：Commit Logging。只有日志记录全部安全落盘，数据库在日志中看到代表事务成功提交的记录后，才会根据日志上的信息对真正的数据进行修改，修改完成后，再在日志中加入一条结束记录，表示事务已完成持久化</li>
<li><strong>影子分页</strong>：Shadow Paging。复制数据副本，在副本上修改数据，接着修改数据引用指针。修改指针可以看作是原子操作</li>
<li><strong>提前写入日志</strong>：Write-Ahead Logging。允许事务提交之前写入数据。<ul>
<li>将何时写入变动数据划分为FORCE和STEAL<ul>
<li><strong>FORCE</strong>：事务提交后，要求变动数据必须同时完成写入则称为FORCE，否则为NO-FORCE。现实中绝大多数数据库采用的都是NO-FORCE策略</li>
<li><strong>STEAL</strong>：事务提交前，允许变动数据提前写入则称为STEAL，不允许则称为NO-STEAL。STEAL策略有利于利用空闲I/O资源，也有利于节省数据库缓存区的内存<ul>
<li>Commit Logging不允许STEAL，Write-Ahead Logging允许，实现方法是增加Undo Log日志，当数据写入磁盘前先记录Undo Log，注明修改了哪个位置的数据、什么值改成什么值，以便出错时Undo</li>
</ul>
</li>
</ul>
</li>
<li>Undo Log加入后，在<strong>崩溃恢复</strong>时会经历以下<strong>三个阶段</strong><ul>
<li><strong>分析阶段Analysis</strong>：从最后一次检查点（这个点之前的所有应该持久化的变动都已经安全落盘）开始扫描日志，找出没有End Record的事务，组成待恢复的事务集合</li>
<li><strong>重做阶段Redo</strong>：根据待恢复的事务集合重演历史，找出包含Commit Record的日志，将数据修改写入磁盘，添加一条End Record，再移出待恢复事务</li>
<li><strong>回滚阶段Undo</strong>：剩下的待恢复事务集合都是需要回滚的事务，根据Undo Log中的信息，回滚Loser事务</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><strong>实现隔离性</strong>：隔离性与并发密切相关<ul>
<li>现代数据库均提供以下三种锁<ul>
<li><strong>写锁</strong>：或排他锁。持有写锁的事务才能对数据进行操作，其他事务不能写入数据，也不能施加读锁（也就是说可以读，但不能加读锁）</li>
<li><strong>读锁</strong>：或共享锁。一个数据可以被加多个读锁，但被加上后就不能加写锁</li>
<li><strong>范围锁</strong>：对于某个范围直接加排他锁，这个范围内数据不能被写入（拥有范围锁的也不能写入，这个范围变成只读的了）</li>
</ul>
</li>
<li><strong>隔离性的级别</strong>：隔离程度与并发能力相互抵触，隔离程度越高，并发访问时吞吐的数据量就越低。现代数据库一定会提供除可串行化以外的其他隔离级别供用户使用，让用户自主调节隔离级别，<strong>根本目的是让用户可以调节数据库的加锁方式，取得隔离性与吞吐量之间的平衡</strong><ul>
<li><strong>串行化访问</strong>：Serializable。提供最高的隔离级别。对事务所有读、写全都加上读锁、写锁、范围锁即可做到串行化（llj：不理解，这句话表述是不是有问题）</li>
<li><strong>可重复读</strong>：Repeatable Read。加读锁和写锁。比串行化弱化的地方在于幻读问题（Phantom Read）<ul>
<li><strong>幻读</strong>：在事务执行过程中，两个完全相同的范围查询得到了不同结果集</li>
</ul>
</li>
<li><strong>读已提交</strong>：Read Committed。对事务涉及的数据加的写锁会一直持续到事务结束，但加的读锁在查询操作后会马上释放。比可重复读弱的地方在于不可重复读问题（Non-Repeatable Read）<ul>
<li><strong>不可重复读</strong>：事务执行过程中，对同一行数据的两次查询得到了不同结果。（llj：两次读分别加锁导致其间的空子被写数据的事务钻了</li>
</ul>
</li>
<li><strong>读未提交</strong>：Read Uncommitted。对事务涉及的数据加写锁，持续到事务结束，但完全不加读锁。比读已提交弱的地方在于脏读（dirty Read）<ul>
<li><strong>脏读</strong>：事务执行过程中，一个事务读取到了另一个事务未提交的数据。（llj：因为不加读锁，所以它可以读到别人加了写锁的数据</li>
</ul>
</li>
<li>完全不隔离：不加锁，脏写</li>
</ul>
</li>
<li>不同的隔离级别以及幻读、不可重复读、脏读等都是表面现象，是各种锁在不同加锁时间上组合应用所产生的结果，<strong>以锁为手段来实现隔离性才是数据库表现出不同隔离级别的根本原因</strong></li>
<li>除了都以锁来事先外，以上四种隔离级别还有一个共同特点，即这些问题都是由于<strong>一个事务在读数据的过程中，受到另一个写数据的事务的影响而破坏了隔离性</strong>，针对<strong>“一个事务读 + 另一个事务写”</strong>的隔离问题，近年来有一种“<strong>多版本并发控制</strong>”（MVCC）的无锁优化方案被主流的商业数据库广泛采用</li>
<li><strong>MVCC</strong>：基本思路是对数据库的任何修改都不会直接覆盖之前的数据，而是新版本与老版本共存，以此达到读取时可以完全不加锁的目的</li>
<li><strong>加锁策略</strong><ul>
<li><strong>悲观加锁策略</strong>：认为如果不先加锁再访问数据，就肯定会出现问题</li>
<li><strong>乐观加锁策略</strong>：认为事务之间数据存在竞争是偶然情况，没有竞争才是普遍情况，这样就不应该在一开始就加锁，而是应当在出现竞争时再找补救措施。这种思路也被称为“乐观并发控制”（Optimistic Concurrency Control，OCC）</li>
</ul>
</li>
</ul>
</li>
<li>XA：略</li>
<li><strong>FLP不可能原理</strong>：如果宕机后不能恢复，那就不存在任何一种分布式协议可以正确地达成一致性结果</li>
<li><strong>CAP定理</strong>：在一个分布式系统中，涉及共享数据问题时，CAP三个特性最多只能同时满足一个<ul>
<li><strong>一致性Consistency</strong>：代表数据在任何时刻、任何分布式节点中所看到的都是符合预期的</li>
<li><strong>可用性Availability</strong>：代表系统不间断提供服务的能力，衡量系统可用时间与总时间之比<ul>
<li>可用性${A = MTBF / (MTBF + MTTR)}$<ul>
<li>可靠性：平均无故障时间MTBF</li>
<li>可维护性：平均可修复时间MTTR</li>
</ul>
</li>
</ul>
</li>
<li><strong>分区容忍性</strong>：代表分布式环境中部分节点因为网络原因而彼此失联后，即与其他节点形成“网络分区”时，系统仍能正确地提供服务的能力</li>
</ul>
</li>
<li>选择放弃一致性的AP系统时目前涉及分布式系统的主流选择</li>
<li>CAP和ACID中讨论的一致性称为“<strong>强一致性</strong>”，也称“<strong>线性一致性</strong>，把牺牲了C的AP系统又要尽可能获得正确结果的行为称为追求“<strong>弱一致性</strong>”（就是“不保证一致性”的意思）。弱一致性中，人们总结出稍微强点的特例，被称为“<strong>最终一致性</strong>”（面向此的算法被称为“<strong>乐观复制算法</strong>”）</li>
<li>事务分类<ul>
<li><strong>刚性事务</strong>：使用ACID的事务称为~</li>
<li><strong>柔性事务</strong>：<ul>
<li>可靠事件队列：使用BASE</li>
<li>TCC事务：Try-Confirm-Cancel</li>
<li>SAGA事务</li>
</ul>
</li>
</ul>
</li>
<li><strong>BASE</strong>：独立于ACID获得的强一致性之外的、使用BASE来达成一致性目的的途径<ul>
<li><strong>基本可用性</strong>：Basically Available</li>
<li><strong>柔性事务</strong>：Soft State</li>
<li><strong>最终一致性</strong>：Eventually Consistent</li>
</ul>
</li>
</ul>
<p>​    </p>
<h2 id="第四章-透明多级分流系统"><a href="#第四章-透明多级分流系统" class="headerlink" title="第四章 透明多级分流系统"></a>第四章 透明多级分流系统</h2><ul>
<li><p>分流时优秀架构设计的一种体现</p>
</li>
<li><p><strong>系统的流量规划的两条原则</strong></p>
<ul>
<li><strong>尽可能减少单点部件</strong>：避免绝大多数流量汇集到单点部件（比如数据库），同时依然能够或者在绝大多数时候保证处理结果的准确性，使单点系统在出现故障时自动而迅速地实施补救措施，这便是<strong>系统架构中多级分流的意义</strong></li>
<li><strong>奥卡姆剃刀原则</strong>：满足需求的前提下，<strong>最简单的系统就是最好的系统</strong></li>
</ul>
</li>
<li><p><strong>透明分流</strong></p>
<ul>
<li><p>价值：获得高可用架构、高并发</p>
</li>
<li><p><strong>工具与手段</strong></p>
<ul>
<li><p><strong>客户端缓存</strong>：HTTP缓存机制</p>
<ul>
<li><strong>状态缓存</strong>：指不经过服务器，客户端直接根据缓存信息对目标网站的状态判断<ul>
<li>301永久重定向</li>
<li>HSTS：HTTP Strict Trnsport Security</li>
</ul>
</li>
<li><strong>强制缓存</strong>：基于时效性的。假设某个时间点到来以前，资源的内容和状态一定不会被改变，因此客户端可以无需经过任何请求，在该时间点前一直持有和使用该资源的本地缓存副本<ul>
<li>Expires：HTTP/1.0开始提供的header，使用绝对时间</li>
<li>Cache-Control：HTTP/1.1定义的，使用相对时间</li>
</ul>
</li>
<li><strong>协商缓存</strong>：基于变化检测的，在一致性上会有比强制缓存更好的表现，但需要一次变化检测的交互开销。协商缓存有两种变动检测机制，分别是<strong>根据资源的修改时间</strong>进行检查，以及<strong>根据资源的唯一标识</strong>是否发生变化进行检查<ul>
<li>Last-Modified和If-Modified-Since：客户端代If-Modified-Since，服务端响应Last-Modified</li>
<li>Etag和If-None-Match：Etag用于告诉客户端这个资源的唯一标识，检查这个标识可以发现文件是否变动<ul>
<li>Etag是HTTP中一致性最强的缓存机制</li>
<li>Etag也是HTTP中性能最差的缓存机制，每次请求，服务端都会对资源进行哈希计算</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>域名解析</strong>：DNS本身就是示范性的透明多级分流系统</p>
</li>
<li><p><strong>传输链路</strong>：</p>
<ul>
<li><strong>连接数优化</strong><ul>
<li><strong>持久连接</strong>：让客户端对同一个域名长期持有一个或多个不会用完即断的TCP连接（llj：TCP连接池）。典型做法是在客户端维护一个FIFO队列，在每次取完数据后一段时间内先不自动断开连接，以便在获取下一个资源时直接复用，避免创建TCP连接的成本</li>
<li><strong>HTTP/2多路复用技术</strong>：HTTP/2中，帧才是最小粒度的信息单位（以前是一个HTTP请求），每个帧附带一个流ID来标识自己属于哪个流（llj：IP-&gt;端口-&gt;流ID）</li>
</ul>
</li>
<li><strong>传输压缩</strong>：即使压缩在内存中边压缩边传输导致Content-Length无法设置，多路复用中无法界定某个资源什么时候传输完毕。HTTP/1.1版本修复了这个问题，增加了“<strong>分块传输编码</strong>”的资源结束判断机制<ul>
<li><strong>分块传输编码</strong>：每个分块中包含独占一行的十六进制长度值和对应长度的数据内容，最后以一个长度值为0的分块表示资源结束</li>
</ul>
</li>
<li><strong>快速UDP网络连接</strong>：QUIC。要从根本上改进HTTP，必需直接替换掉HTTP over TCP的根基，即TCP，这是最新一代HTTP/3协议的设计重点<ul>
<li>QUIC可靠性由自身实现</li>
<li>QUIC提供面向移动设备的专门支持，如果使用TCP，移动网络和Wifi互切时，TCP必将超时然后重新创建。QUIC提出标识符的概念（llj：cookie），唯一地标识客户端与服务端之间的连接，而无需依靠IP地址。这样，切换网络后，只需要向服务器发送一个包含此标识符的数据包即可宠用既有的连接</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>内容分发网络</strong>：内容分发网络的工作过程，主要涉及路由解析、内容分发、负载均衡和CDN应用四个方面</p>
<ul>
<li><strong>路由解析</strong>：CDN将用户请求路由到它的资源服务器上就是依靠DNS服务器来实现的<ul>
<li>在CDN服务器上将自己服务器IP注册为源站，获得一条CNAME，将CNAME加入到自己服务器的域名提供商CNAME记录中。客户访问时，DNS返回CNAME，本地DNS再解析CNAME，由于能解析该CNAME的权威服务器只有CDN服务商架构的DNS，CDN服务商的DNS会挑一个最合适的IP替代源站IP返回，客户端从该IP获取内容</li>
</ul>
</li>
<li><strong>内容分发</strong>：<ul>
<li>两种主流的内容分发方式<ul>
<li><strong>主动分发Push</strong>：分发由源站主动发起，将内容推送到CDN节点上</li>
<li><strong>被动回源Pull</strong>：用户访问后发生缓存不命中，balabala</li>
</ul>
</li>
<li>CDN如何管理（更新）资源？两种方法相结合<ul>
<li><strong>超时被动失效</strong>：给予缓存资源一定的生存期，超过了生存期就在下次请求时重新被动回源一次</li>
<li><strong>手工主动失效</strong>：网站更新时，由持续集成的流水线自动调用CDN提供处理缓存失效的接口来实现缓存更新</li>
</ul>
</li>
</ul>
</li>
<li><strong>CDN应用</strong><ul>
<li><strong>加速静态资源发布</strong>：CDN的本职工作</li>
<li><strong>安全防御</strong>：源站的堡垒机</li>
<li><strong>协议升级</strong>：http转https，http协议升级，IPv4转IPv6……</li>
<li><strong>状态缓存</strong></li>
<li><strong>修改资源</strong>：压缩资源等</li>
<li><strong>访问控制</strong>：控制不同IP的访问</li>
<li><strong>注入功能</strong></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>负载均衡</strong>：调度后方的多台机器，以统一的接口对外提供服务</p>
<ul>
<li><strong>四层负载均衡</strong>：这些工作模式的共同特点是维持同一个TCP连接，而不是只工作在第四层<ul>
<li><strong>数据链路层负载均衡</strong>：修改包的目的MAC地址，服务器和负载均衡器必须在同一个子网中，且IP都与负载均衡器相同（因为这个包里的目的IP地址是负载均衡器的，服务器必须核对自己的IP地址和目的IP地址是否相同</li>
<li><strong>网络层负载均衡</strong><ul>
<li><strong>将原数据包包装到新数据包内</strong>（IP隧道），发到服务器上，服务器再拆包（服务器IP和负载均衡器IP相同，因为回应时，需要将源IP设置与负载均衡器相同）</li>
<li><strong>直接修改数据包的目的IP</strong>，又称<strong>NAT模式</strong>，负载均衡器就像家里的路由器，提供NAT功能。负载均衡器修改发往服务器的数据包的目的IP，修改从服务器收到的数据包的源IP</li>
</ul>
</li>
</ul>
</li>
<li><strong>七层负载均衡</strong>：应用层负载均衡，反向代理的一种，由负载均衡器建立两条TCP连接，对客户和服务器的交流传话</li>
<li><strong>均衡策略与实现</strong><ul>
<li><strong>策略</strong><ul>
<li>轮询均衡</li>
<li>权重轮询均衡</li>
<li>随机均衡</li>
<li>权重随机均衡</li>
<li>一致性哈希均衡</li>
<li>响应速度均衡</li>
<li>最少连接数均衡</li>
</ul>
</li>
<li><strong>实现</strong><ul>
<li><strong>软件均衡器</strong><ul>
<li><strong>OS内核的均衡器</strong><ul>
<li>LVS：Linux Virtual Server</li>
</ul>
</li>
<li><strong>应用程序均衡器</strong><ul>
<li>Nginx</li>
<li>HAProxy</li>
<li>……</li>
</ul>
</li>
</ul>
</li>
<li><strong>硬件均衡器</strong>：往往采用专用集成电路来实现，以达到最高的性能</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>服务端缓存</strong>：引入缓存首先要考虑的是是否真的需要缓存</p>
<ul>
<li>在软件开发中引入缓存的负面作用要明显大于硬件缓存带来的负面作用<ul>
<li>开发角度：会提高系统复杂度，因为要考虑缓存的失效、更新、一致性等问题</li>
<li>运维角度：缓存会掩盖一些缺陷，让问题在更久的时间以后，出现在距离发生现场更远的位置上</li>
<li>安全角度：缓存可能会泄露某些保密数据，也是容易收到攻击的薄弱点</li>
</ul>
</li>
<li><strong>冒着这些风险，仍然能说服你引入缓存的理由</strong>：这里的言外之意是，如果可以通过增强CPU、I/O本身的性能来满足需求的话，那升级硬件往往是更好的解决方案，即使需要一些额外的投入成本，也通常要优于引入缓存后可能带来的风险<ul>
<li>为缓解CPU压力而引入缓存</li>
<li>为缓解I/O压力而引入缓存</li>
</ul>
</li>
<li><strong>缓存属性</strong>：设计或者选择缓存至少会考虑的缓存属性<ul>
<li><strong>吞吐量</strong></li>
<li><strong>命中率</strong></li>
<li><strong>扩展功能</strong></li>
<li><strong>分布式缓存</strong></li>
</ul>
</li>
<li><strong>缓存风险</strong><ul>
<li><strong>缓存穿透</strong>：查询不存在的数据导致每次都不命中，且都会触及末端的数据库<ul>
<li>缓存不存在的对象（空值）</li>
<li>设置布隆过滤器</li>
</ul>
</li>
<li><strong>缓存击穿</strong>：某些热点数据忽然失效，此时又有多个针对该数据的请求同时到达，全部击穿缓存到达末端数据库，导致数据源压力剧增<ul>
<li>加锁同步</li>
<li>热点数据由代码手动管理</li>
</ul>
</li>
<li><strong>缓存雪崩</strong>：大批不同的数据在短时间内一起失效，导致对这些数据的请求直接到达数据源，导致……<ul>
<li>提升缓存可用性，建设分布式缓存</li>
<li>启用透明多级缓存</li>
<li>缓存的生存期从固定时间改为一个时间段内的随机时间</li>
</ul>
</li>
<li><strong>缓存污染</strong>：指缓存中数据和真实数据源中数据不一致的现象</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="第五章-架构安全性"><a href="#第五章-架构安全性" class="headerlink" title="第五章 架构安全性"></a>第五章 架构安全性</h2><ul>
<li><strong>认证</strong>：Authentication。你是谁？<ul>
<li><strong>一个架构安全性的经验原则</strong>：以标准规范为指导、以标准接口去实现，严格遵循标准，就是最恰当的设计</li>
<li><strong>主流的三种认证方式</strong><ul>
<li><strong>通信信道上的认证</strong>：建立通信连接之前的认证（SSL/TLS）</li>
<li><strong>通信协议上的认证</strong>：如基于HTTP协议的认证，由HTTP协议本身提供支持</li>
<li><strong>通信内容上的认证</strong>：如基于Web内容的认证，由系统开发者自己实现</li>
</ul>
</li>
</ul>
</li>
<li><strong>授权</strong>：Authorization<ul>
<li><strong>涉及问题</strong><ul>
<li><strong>确保授权的过程可靠</strong></li>
<li><strong>确保授权的结果可控</strong></li>
</ul>
</li>
<li><strong>常用的两种方案</strong><ul>
<li><strong>RBAC</strong>：Role-Based Access Control，基于角色的访问控制<ul>
<li>RBAC将权限从用户身上剥离，改为绑定到角色上</li>
</ul>
</li>
<li><strong>OAuth 2</strong>：面向解决第三方应用的认证授权协议<ul>
<li>以令牌代替用户密码吗作为授权的凭证</li>
<li>四种授权方式<ul>
<li>授权码模式</li>
<li>隐式授权模式</li>
<li>密码模式</li>
<li>客户端模式</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>建立访问控制模型的基本目的是管理垂直权限和水平权限<ul>
<li><strong>垂直权限</strong>：功能权限，哪些角色可以使用那些功能</li>
<li><strong>水平权限</strong>：数据权限，同角色之间产生的数据怎么隔离</li>
</ul>
</li>
</ul>
</li>
<li><strong>凭证</strong>：Credential<ul>
<li><strong>Cookie-Session</strong>：将状态保存在服务端，在分布式环境会遇到CAP不可兼得的问题</li>
<li><strong>JWT</strong>：将状态保存在客户端，是目前广泛使用的一种令牌格式。JWT只解决篡改问题，不解决泄漏问题，因此令牌默认不加密<ul>
<li><strong>令牌头</strong>：描述了令牌的类型（统一为typ:JWT）以及令牌签名的算法</li>
<li><strong>负载</strong>：令牌真正需要向服务器端传递的信息。推荐了七种申明名称<ul>
<li>iss：Issuer，签发人</li>
<li>exp：Expiration Time，过期时间</li>
<li>sub：Subject，主题</li>
<li>aud：Audience，令牌受众</li>
<li>nbf：Not Before，令牌生效时间</li>
<li>iat：Issued At，令牌签发时间</li>
<li>jti：JWT ID，令牌编号</li>
</ul>
</li>
<li><strong>签名</strong>：使用在令牌头中公开的特定签名算法，服务端对前面两部分签名</li>
</ul>
</li>
</ul>
</li>
<li><strong>保密</strong>：Confidentiality，端的保密、链路的保密<ul>
<li>做系统设计时就应该把明文密码这种东西当成是最烫手的山芋来看待，越早消灭越好，将一个潜在的炸弹从客户端运到服务端，对绝大多数系统来说都没有必要</li>
<li>真正防御性的密码加密存储确实应该在服务端中进行，但这是为了降低服务端被攻破而批量泄露密码的风险，并不是为了增加传输过程的安全</li>
<li><strong>慢哈希函数</strong>：指执行时间可以调节的哈希函数，通常以控制调用次数来实现</li>
</ul>
</li>
<li><strong>传输</strong>：Transport Security<ul>
<li>现代密码学算法的三种主要用途<ul>
<li>摘要</li>
<li>加密</li>
<li>签名</li>
</ul>
</li>
<li>由多种加密算法组合的应用形式称为<strong>密码学套件</strong>，非对称加密在混合加密种发挥的作用称为<strong>密钥协商</strong></li>
<li>X.509标准格式证书格式<ul>
<li><strong>版本号</strong>：指出该证书使用了哪种X.509标准</li>
<li><strong>序列号</strong>：Serial Number，证书唯一标识符</li>
<li><strong>签名算法标识符</strong>：Signature Algorithm，用于签发证书的算法标识</li>
<li><strong>认证机构的数字签名</strong>：证书发布者私钥对证书的签名</li>
<li><strong>认证机构</strong>：Issuer，证书颁发者的可识别名</li>
<li><strong>有效期限</strong>：Validity</li>
<li><strong>主题信息</strong>：Subject，证书持有人唯一的标识符，通常使用的是网站的域名</li>
<li><strong>公钥信息</strong>：包括证书持有人的公钥，密钥所属的密码体系的标识符和其他参数</li>
</ul>
</li>
</ul>
</li>
<li><strong>验证</strong>：Verification<ul>
<li>提倡把校验行为从分层中剥离出来，不是在哪一层上做，而是在Bean上做</li>
</ul>
</li>
</ul>
<h1 id="第三部分-分布式的基石"><a href="#第三部分-分布式的基石" class="headerlink" title="第三部分 分布式的基石"></a>第三部分 分布式的基石</h1><h2 id="第六章-分布式共识"><a href="#第六章-分布式共识" class="headerlink" title="第六章 分布式共识"></a>第六章 分布式共识</h2><ul>
<li><strong>数据复制方法</strong><ul>
<li><strong>状态转移</strong>：以同步为代表的数据复制方法。每当数据发生变化，把变化情况在各个节点间的复制视作一种事务性的操作，只有系统里每一台机器都反馈成功、完成磁盘写入后，数据的变化才宣告成功。通常要牺牲可用性</li>
<li><strong>操作转移</strong>：状态机，通过某种操作，令源状态转换为目标状态，分布式系统里的主流数据复制方法<ul>
<li><strong>状态机复制</strong>：在广播指令和指令执行期间，允许系统内部状态存在不一致的情况，即并不要求所有节点的每一条指令都是同时开始、同步完成的，只要求在此期间的内部状态不能被外部观察到，且当操作指令序列执行完毕时，所有节点的最终状态是一致的，这种模型就被称为~</li>
</ul>
</li>
</ul>
</li>
<li><strong>Quorum机制</strong>：考虑到分布式环境下网络分区现象是不可能消除的，放弃对C的追求，而是采用“少数服从多数”的原则，一旦系统中过半的节点完成了状态的转换，就认为数据的变化已经被正确地存储在了系统中，这样就可以容忍少数节点的失联，减弱增加机器数量对系统整体可用性的影响，这种思想在分布式中被称为~</li>
<li><strong>Paxos</strong>：一种基于消息传递的协商共识算法，是当今分布式系统最重要的理论基础，几乎就是“共识”二字的代名词<ul>
<li>将分布式系统中的节点分为三类，所有节点都是平等的，他们都可以承担一下某一种或多种角色<ul>
<li><strong>提案节点</strong>：Proposer，提出对某个值进行设置操作的节点</li>
<li><strong>决策节点</strong>：Acceptor，应答提案的节点，决定该提案是否可被投票、接受，<strong>提案一旦得到超过半数决策节点的接受，即称该提案被批准（Accept）</strong>，被批准意味着该值不能被更改，不会丢失，且最终所有节点都会接受它</li>
<li><strong>记录节点</strong>：Learner，单纯地从提案、决策节点中学习已经达成共识的提案</li>
</ul>
</li>
<li>分布式环境下，一致性问题的复杂度主要来源于两个方面因素<ul>
<li>系统内部各个节点通信是不可靠的</li>
<li>系统外部各个用户访问是并发的<ul>
<li>为了解决这个问题，分布式环境中的锁必须是可抢占的</li>
</ul>
</li>
</ul>
</li>
<li>过程<ul>
<li><strong>准备Prepare</strong>：相当于抢占锁的过程<ul>
<li>提案节点向所有决策节点广播一个许可申请（Prepare请求），并附带一个全局唯一且单调递增的数字n作为提案ID，决策节点收到后，将会给予提案节点两个承诺与一个应答<ul>
<li>两个承诺<ul>
<li>承诺不会再接受提案ID小于或等于n的Prepare请求</li>
<li>承诺不会再接受提案ID小于n的Accept请求</li>
</ul>
</li>
<li>一个应答<ul>
<li>不违背承诺的前提下，回复已经接受的ID最大对应的ID和值，如果该值没被设定过，返回空值</li>
<li>如果违反承诺，即收到的提案ID并不是收到过的最大ID，那允许不理会</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><strong>批准Accept</strong>：提案节点收到了多数派决策节点的应答（Promise应答）后，就开始<ul>
<li>如果响应的决策节点都返回为空，则提案节点是第一个设置值得节点，根据自己选定的值，将二元组(id, value)，广播给全部决策节点</li>
<li>如果至少有一个不为空，则不能随意取值，而必须找出提案ID最大的那个值并接受，组成二元组(id, maxAcceptValue)，再次广播给全部决策节点</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><strong>Multi Paxos</strong>：Multi Paxos对Basic Paxos的核心改进是增加了“<strong>选主</strong>”的过程，提案节点会通过定时轮询（心跳），确定当前网络中所有节点是否存在一个主提案节点，一旦发现没有主节点，节点就会在心跳超时后使用Basic Paxos中定义的准备、批准的两轮网络交互过程，向所有其他节点广播自己希望竞选主节点的请求，希望分布式系统对“有我作为主节点”这件事协商达成一致共识，如果得到了决策节点中多数派的批准，便宣布竞选成功，此后其他节点的提案都转发给主节点来完成（llj：这相当于大学选个班长把全班的事都做了，累死了再选一个）</li>
<li><strong>Raft</strong>：以下三个问题同时被解决时，即等价于达成共识<ul>
<li>选主</li>
<li>复制</li>
<li>安全</li>
</ul>
</li>
<li>Raft、ZAB（ZooKeeper的）都被认为时Multi Paxos的派生实现</li>
<li><strong>Gossip协议</strong>：一种用于分布式数据库在多节点间复制同步数据的算法<ul>
<li>Gossip具体工作过程可以看作以下两个步骤的简单循环<ul>
<li>如果由一项信息需要在网络中所有节点中传播，那从信息源开始，选择固定的传播周期，随机选择与它相连接的k个节点（称为Fan-Out）来传播消息</li>
<li>每个节点收到消息后，如果这个消息是它没收到过的，则在下一个周期内，向除了发送给它消息的那个节点外的其他k个相邻节点发送相同信息</li>
</ul>
</li>
<li>达成一致性耗费的时间与网络传播中消息冗余量这两个缺点存在一定对立，如果改善其中一个，必定恶化另一个</li>
</ul>
</li>
</ul>
<h2 id="第七章-从类库到服务"><a href="#第七章-从类库到服务" class="headerlink" title="第七章 从类库到服务"></a>第七章 从类库到服务</h2><ul>
<li>微服务架构的一个重要设计原则是“通过服务来实现独立自治的组件”（Componentization via Service），强调应采用“服务”而不是“类库（Library）”来构建组件化程序</li>
<li>三个问题和解决方案<ul>
<li>消费者：外部服务由谁来提供？在什么位置？（<strong>服务发现</strong>）</li>
<li>生产者：内部那些服务需要暴露？哪些需要隐藏？应以何种形式暴露服务？以什么规则在集群中分配请求？（<strong>服务网关路由</strong>）</li>
<li>调用过程：如何保证每个远程服务都接受到相对平均的流量，获得尽可能高的服务质量与可靠性？（<strong>服务的负载均衡</strong>）</li>
</ul>
</li>
<li><strong>服务发现</strong>：如何确定目标方法的确切位置，便是与编译连接（相比类库）有着同等意义的研究课题，解决该问题的过程被称为~<ul>
<li>所有远程调用都是使用<strong>全限定名（Fully Qualified Domain Name，FQDN）</strong>、端口号和服务标识所构成的三元组来确定一个远程服务的精确坐标</li>
<li>两种不同理解<ul>
<li>UDDI为代表的百科全书式的服务发现，上至提供服务的企业信息，下至服务的程序接口细节都在服务发现管辖范围内</li>
<li>类似DNS这样的门牌号码式的服务发现，只满足从偶u个代表服务提供者的全限定名到服务实际主机IP地址的翻译转换</li>
</ul>
</li>
<li>如何在基础设施和网络协议层面，对应用尽可能无感知、方便地实现服务发现是目前服务发现地一个主要发展方向</li>
<li><strong>三个必须的过程</strong><ul>
<li><strong>服务的注册</strong>：当服务启动的时候，它应该通过某些形式将自己的坐标信息通知到服务注册中心<ul>
<li>自注册模式：应用程序本身去完成</li>
<li>第三方注册模式：譬如Kubernetes 和Registrator</li>
</ul>
</li>
<li><strong>服务的维护</strong>：服务发现框架必须自己保证所维护的服务列表的正确性，以免告知消费者服务的坐标后，得到的服务却不能使用的尴尬情况。监控服务是否健康存活，将不健康的服务自动从注册表中剔除</li>
<li><strong>服务的发现</strong>：特指狭义上消费者从服务发现框架中，把一个符号转换成服务实际坐标的过程</li>
</ul>
</li>
<li>注册中心的实现<ul>
<li>在分布式K/V存储框架上自己开发的服务发现：典型代表是ZooKeeper、Doozerd、etcd</li>
<li>以基础设施来实现服务发现：主要指DNS服务器，典型代表是SkyDNS、CoreDNS</li>
<li>专门用于服务发现的框架和工具：典型代表是Eureka、Consul、Nacos</li>
</ul>
</li>
</ul>
</li>
<li><strong>网关路由</strong>：网关用于标识位于内部区域边缘，与外界进行交互的某个物理或逻辑设备<ul>
<li>微服务中网关的首要<strong>职责</strong>就是作为统一的出口对外提供服务，将外部访问网关地址的流量，根据适当的规则路由到内部集群中正确的服务节点之上，因此，微服务中的网关，也常被称为“服务网关”或“API网关”，微服务中的网关首先应该是个路由器</li>
<li>网关 = 路由器（基础职能） + 过滤器（可选职能）</li>
<li>网关必须能够识别流量中的特征者意味着网关你能够支持的网络通信协议的层次将会直接限制后端服务节点能够选择的服务通信方式</li>
<li><strong>网络I/O模型</strong>：“等待数据从远程主机到达缓冲区”、“将数据从缓冲区复制到应用程序的地址空间”，根据者两阶段将模型分为两类五种<ul>
<li><strong>异步I/O</strong></li>
<li><strong>同步I/O</strong><ul>
<li><strong>阻塞I/O</strong></li>
<li><strong>非阻塞I/O</strong></li>
<li><strong>多路复用I/O</strong>：在同一条阻塞线程上处理多个不同端口的监听，<strong>是目前高并发网络应用的主流，它还可以细分为select、epoll、kqueue等不同实现</strong></li>
<li><strong>信号驱动I/O</strong>：收到信号之后自己从缓冲区复制数据</li>
</ul>
</li>
</ul>
</li>
<li><strong>BFF网关</strong>：Backend for Frontend。网关不必为所有前端提供无差别服务，而是应该针对不同前端，聚合不同服务，提供不同接口和网络访问协议支持（llj：网关微服务化？外观模式？）</li>
</ul>
</li>
<li><strong>代理负载均衡器</strong>：将负载均衡器提取出来，作为同一个进程之外，同意给Pod之内的特殊服务，放到边车代理中实现</li>
</ul>
<h2 id="第八章-流量治理"><a href="#第八章-流量治理" class="headerlink" title="第八章 流量治理"></a>第八章 流量治理</h2><ul>
<li>容错性设计是微服务的另一个核心原则</li>
<li>熔断、隔离、重试、降级、超时等概念都是建立具有韧性的微服务系统所必须的保留措施</li>
<li>面对能够自动扩缩的大型分布式系统，静态的配置越来越难以起到良好的效果，这就需要系统不仅要有能力自动根据服务负载来调整服务器的数量规模，还要有能力根据服务调用的统计结果，互殴这启发式搜索的结果来自动变更容错策略和参数，这方面研究还处于初级阶段，时服务治理的未来重要发展方向之一</li>
<li>两个问题<ul>
<li>某个服务崩溃，导致所有依赖该服务的服务无法正常工作</li>
<li>服务没有崩溃，但是处理能力有限。如果一开始的堵塞没有得到及时的治理，后面就需要很长时间来使全部服务恢复正常</li>
</ul>
</li>
<li><strong>服务容错</strong>：原本信息系统设计成分布式架构的主要动力之一就是为了提升系统的可用性，最低限度也必须保证系统重构后可用性不下降才行<ul>
<li><strong>常见的容错策略</strong>：面对故障，我们该做什么<ul>
<li><strong>故障转移</strong>：Failover。如果调用的服务器出现故障，系统不会立即向调用者返回失败结果，而是自动切换到其他服务副本，尝试通过其他副本返回成功调用的结果，从而保证整体的高可用性</li>
<li><strong>快速失败</strong>：Failfast。尽快让服务报错，坚决避免重试，尽快抛出异常，让调用者自行处理</li>
<li><strong>安全失败</strong>：Failsafe。并不是每个服务都是不可或缺的，有部分服务失败了也不影响核心业务的正确性（比如日志输出）。对此，一种理想的容错策略是即使旁路逻辑实际调用失败了，也当作正确值返回，如果需要返回值，系统就自动返回要给符合要求的数据类型的对应零值，然后自动记录一条服务调用出错的日志备查即可，这种策略被称为~</li>
<li><strong>沉默失败</strong>：Failsilent。请求失败后，就默认服务提供者一定时间内无法再对外提供服务，不再向它分配请求流量</li>
<li><strong>故障恢复</strong>：Failback。一般作为其他容错策略的补充。当服务调用出错之后，将该次调用失败的信息存入一个消息队列中，然后由系统自动开始异步重试调用</li>
<li><strong>并行调用</strong>：Forking。一开始就同时向多个服务副本发起调用，只要其中任何一个返回成功，那调用就宣告成功</li>
<li><strong>广播调用</strong>：Broadcast。与并行调用相对立。要求所有调用全部成功，这次调用才算成功。<strong>常用于实现“刷新分布式缓存”这类操作</strong></li>
</ul>
</li>
<li><strong>常见的容错设计模式</strong>：要实现某种容错策略，我们该如何去做<ul>
<li><strong>断路器模式</strong>：通过代理一对一接管服务调用者的远程请求。断路器会持续监控并统计服务返回的各种结果，当出现故障的次数超过设定的阈值时，它的状态就变为“OPEN”，后续所有的代理访问将直接返回失败，而不发出真正请求。<ul>
<li>断路器的作用是自动进行服务熔断，这是一种<strong>快速失败</strong>的容错策略的实现方法</li>
</ul>
</li>
<li><strong>舱壁隔离模式</strong>：常用的实现服务隔离的设计模式。为服务分配资源上限，为每个服务设置线程池上限，防止一个服务占用了所有资源，因一个服务瘫痪导致整个系统瘫痪。<strong>沉默失败</strong>的容错策略的实现方法</li>
<li><strong>重试模式</strong>：实现<strong>故障转移</strong>和<strong>故障恢复</strong><ul>
<li>实践中，重试模式面临的风险反而大多源于太过简单而导致的滥用，使用时最好满足以下条件<ul>
<li>尽在主路逻辑的关键服务上进行同步的重试，非关键的服务一般不把重试作为首选的容错方案，尤其不该进行同步的重试</li>
<li>仅对由瞬时故障导致的失败进行重试</li>
<li>仅对具备幂等性的服务进行重试</li>
<li>重试必须有明确的终止条件<ul>
<li>超时终止</li>
<li>次数终止</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><strong>流量控制</strong>：<ul>
<li><strong>流量整形</strong>：Traffic Shaping。描述如何限制网络设备的流量突变，使得网络报文以比较均匀的速度向外发送。流量整形通常都需要用到缓冲区来实现。当报文发送速度过快时，首先在缓冲区中暂存，然后在控制算法的调节下均匀地发送这些被缓冲的报文，常用的由漏桶算法、令牌桶算法</li>
<li>一个健壮的系统需要做到恰当的流量控制，具体解决<strong>三个问题</strong><ul>
<li>依据什么限流</li>
<li>具体如何限流</li>
<li>超额流量如何处理<ul>
<li>直接返回错误</li>
<li>否决式限流：迫使他们进入降级逻辑</li>
<li>阻塞式限流：让多余请求排队等待</li>
</ul>
</li>
</ul>
</li>
<li><strong>流量统计指标</strong>：经常用于衡量服务流量压力的。整体上我们希望用TPS来限流，目前主流系统大多倾向使用HPS作为首选的限流指标，它是相对容易观察并统计的，而且能够在一定程度上翻译系统当前以及接下来一段时间的压力<ul>
<li><strong>TPS</strong>：Transaction per Second，每秒事务数。用于衡量信息系统吞吐量的<strong>最终标准</strong></li>
<li><strong>HPS</strong>：Hit per Second，每秒请求数。指每秒从客户端发向服务端的请求数</li>
<li><strong>QPS</strong>：Query per Second，每秒查询数。指一台服务器能够响应的查询次数。如果只有一台服务器来应答请求，那QPS和HPS时等价的，但在分布式系统中，一个请求的响应往往要由后台多个服务节点共同协作来完成</li>
</ul>
</li>
<li><strong>限流设计模式</strong>：常用的四种<ul>
<li><strong>流量计数器模式</strong>：设置一个计数器，根据当前时刻的流量计数结果是否超过阈值来决定是否限流。缺陷根源在于它只是针对时间点进行离散的统计</li>
<li><strong>滑动时间窗模式</strong>：llj：相当于加了缓冲的多指标计数器模式</li>
<li><strong>漏桶模式</strong>：以适当速率直接放行缓冲中的流量</li>
<li><strong>令牌桶模式</strong>：以适当速率发放令牌（流量通行证）</li>
</ul>
</li>
<li><strong>分布式限流</strong>：前面的限流模式统称为<strong>单机限流</strong>，把能够精细控制分布式集群中每个服务消耗量的限流算法称为~<ul>
<li>一种常见的简单分布式限流方法是将所有服务的统计结果都存入集中式缓存（如Redis）中，以实现在集群内共享，并通过分布式锁、信号量等机制，解决这些数据读写访问时并发控制的问题</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="第九章-可靠通信"><a href="#第九章-可靠通信" class="headerlink" title="第九章 可靠通信"></a>第九章 可靠通信</h2><ul>
<li>微服务提倡分散治理（Decentralized Governance），不追求统一的技术平台，提倡让团队有自由选择的权力，不受制于语言和技术的框架</li>
<li><strong>零信任网络</strong>：<ul>
<li><strong>基于边界的安全模型</strong>：Perimeter-Based Security Model。主流的安全观念提倡根据某类与宿主机相关的特征（如位置、IP、子网等），把网络划分为不同的区域，不同区域对应不同的风险级别和允许访问的网络资源权限，将安全防护措施集中部署在个各个区域的边界之上，重点关注跨区域的网络流量，这种安全模型被称为~</li>
<li><strong>零信任安全模型</strong>：Zero-Trust Security Model。单纯的边界安全已不足以满足大规模微服务系统技术异构和节点膨胀的发展需要。<strong>中心思想</strong>是不应当以某种固有特征来自动信任任何流量，除非明确得到了能代表请求来源的身份凭证，否则一律不会有默认的信任关系</li>
<li>Google认为零信任安全模型的最终目标是实现整个基础设施之上的自动化安全控制，服务所需的安全能力可以与服务自身一起，以相同方式自动进行伸缩扩展</li>
<li>零信任安全模型在引入了比边界安全更细致、更复杂的安全措施的同时，也强调自动与透明的重要性，既要保证系统各个微服务之间能安全通信，也要保证不削弱微服务架构本身的设计原则</li>
<li><strong>某些观点</strong><ul>
<li><strong>零信任网络不等同于放弃在边界上的保护措施</strong></li>
<li><strong>身份只来源于服务</strong>：身份只能来源于服务本身所能够出示的身份凭证（通常时数字证书），而不是服务所在的IP地址、主机名或者其他特征</li>
<li><strong>服务之间没有固定的信任关系</strong>：这点决定了只有已知的、明确授权的调用者才能访问服务阻止攻击者通过某个服务节点中的代码漏洞来越权调用其他服务</li>
<li><strong>集中、共享的安全策略实施点</strong>：分散治理，但涉及安全的非功能需求最好除外。安全需求应该从微服务的应用代码下沉至云原生的基础设施里</li>
<li><strong>受信的机器运行来源已知的代码</strong>：限制服务只能使用认证过的代码和配置，并且只能运行在认证过的环境中</li>
<li><strong>自动化、标准化的变更管理</strong>：做不到标准化意味着做不到自动化。一套独立于应用的安全基础设施，可以让运维人员轻松了解基础设施变更管对安全的影响，也可以在几乎不影响生产环境的情况下发布安全补丁程序</li>
<li><strong>强隔离性的工作负载</strong></li>
</ul>
</li>
</ul>
</li>
<li><strong>服务安全</strong>：<ul>
<li>零信任网络里不存在默认的信任关系，一切服务调用、资源访问成功与否，均需以调用者与提供者间已建立的信任关系为前提</li>
<li>认证<ul>
<li><strong>服务认证</strong>：Peer Authentication。以机器为认证对象，即访问的流量来源于另一个服务</li>
<li><strong>请求认证</strong>：Request Authentication。以人类为对象，即访问的流量来自于最终用户</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="第十章-可观测性"><a href="#第十章-可观测性" class="headerlink" title="第十章 可观测性"></a>第十章 可观测性</h2><ul>
<li><p><strong>可观测性</strong>：可以由外部输出推断其内部状态的程度</p>
<ul>
<li><p><strong>终极解决方案</strong>：OpenTelemetry</p>
</li>
<li><p><strong>三个具体的方向</strong></p>
<ul>
<li><p><strong>事件日志</strong>：主流ELK</p>
<ul>
<li><strong>日志</strong>：Logging。日志的职责是记录离散事件，通过这记录分析出程序的行为</li>
<li>日志中常见的不该有的例子<ul>
<li><strong>避免打印敏感信息</strong></li>
<li><strong>避免引用慢操作</strong>：日志中打印的信息应该是在上下文中可以直接取到的，如果当前上下文中根本没有这项数据，就不要去获取再打出来</li>
<li><strong>避免打印追踪诊断信息</strong>：日志中不要打印方法输入参数、输出结果、方法执行时长之类的调试信息。这个观点是反直觉的，不少公司甚至会将其作为最佳实践来提倡（可能隐藏敏感信息、可能引起慢操作，因为参数可能是一个超大的嵌套对象）。<strong>追踪诊断信息应该由追踪系统去处理</strong></li>
<li><strong>避免误导他人</strong></li>
</ul>
</li>
<li>日志中常见的不应该少的例子<ul>
<li><strong>处理请求时的TraceID</strong>：服务收到请求时，如果该请求没有附带TraceID，就应该自动生成唯一的TraceID来对请求进行标记，并且使用MDC（映射诊断上下文）自动输出到日志。<ul>
<li>TraceID会贯穿整条调用链，<strong>目的</strong>是通过它把请求在分布式系统各个服务中的执行过程串联起来。TraceID通常也会随着请求的响应返回到客户端，如果响应内容出现了异常，用户便能通过此ID快速找到与问题相关的日志</li>
<li>TraceID时链路追踪里的概念，类似的还有<strong>用于标识进程内调用状况的SpanID</strong></li>
</ul>
</li>
<li><strong>系统运行过程中的关键事件</strong></li>
<li><strong>启动时输出配置信息</strong>：将非敏感的配置信息输出到日志</li>
</ul>
</li>
<li>收集与缓冲 -&gt; 加工与聚合 -&gt; 存储与查询</li>
<li>Logstash的基本职能：把日志行中的非结构化数据，通过Grok表达式语法转换为结构化数据</li>
</ul>
</li>
<li><p><strong>链路追踪</strong></p>
<ul>
<li><strong>追踪</strong>：Tracing。<ul>
<li>单体系统时代追踪的范畴基本只局限于栈追踪（Stack Tracing），例如调用调试程序时，在IDE打个断点，看到调用栈视图上的内容便是追踪。</li>
<li>微服务时代，追踪就不局限于与调用栈了，一个外部请求需要内部若干服务的联动响应，这时候完整的调用轨迹将跨越多个服务，同时包括服务间的网络传输信息与各个服务内部的调用堆栈信息，因此，分布式系统中的追踪在国内常被称为“全链路追踪”、“分布式追踪”。</li>
<li><strong>主要目的</strong>：排查故障，如分析调用链的哪一部分、哪个方法出现错误或阻塞，输入输出是否符合预期等等</li>
<li>各个服务之间是使用HTTP还是gRPC来进行通信会直接影响追踪的实现，这种特性决定了追踪工具本身有较强的侵入性，通常是以插件式的探针来实现的</li>
</ul>
</li>
<li>广义上，一个完整的分布式追踪系统应该由三个相对独立的子系统构成<ul>
<li>数据收集</li>
<li>数据存储</li>
<li>数据展示</li>
</ul>
</li>
<li>狭义上，追踪特指链路追踪数据的收集部分</li>
<li><strong>Dapper提出的两个概念</strong><ul>
<li><strong>追踪</strong>：Trace。从客户端发起请求到达系统的边界开始，记录请求流经的每一个服务，指导向客户端返回响应为止</li>
<li><strong>跨度</strong>：Span。由于每次Trace都可能会调用数量不定、坐标不定的多个服务，未来能够记录具体调用了哪些服务，以及调用顺序、开始时点、执行时长等信息，每次开始调用服务前都要先埋入一个调用记录，这个记录称为一个跨度</li>
</ul>
</li>
<li><strong>挑战</strong><ul>
<li>功能上：源于服务的异构性</li>
<li>非功能：<ul>
<li><strong>低性能损耗</strong>：分布式追踪不能对服务本身产生明显的性能负担</li>
<li><strong>对应用透明</strong>：追踪系统通常是运维期才加入的系统，应该尽量以非侵入式或者少侵入的方式来实现追踪，对开发人员做到透明化</li>
<li><strong>随应用扩缩</strong>：现代的分布式服务集群都有根据流量压力自动扩缩的能力</li>
<li><strong>持续的监控</strong>：追踪系统必须全天工作</li>
</ul>
</li>
</ul>
</li>
<li>根据数据收集方式的差异，可分为三种主流的实现方式<ul>
<li><strong>基于日志的追踪</strong>：Log-Based Tracing。将Trace、Span等信息直接输出到应用日志中，然后所有节点的日志归集过程汇集到一起，再从全局日志信息中反推出完整的调用链拓扑关系（代表产品：Spring Cloud Sleuth）</li>
<li><strong>基于服务的追踪</strong>：Service-Based Tracing。<strong>目前最常见的追踪方式</strong>，被Zipkin、SkyWalking、Pinpoint等主流追踪系统广泛采用。服务追踪的实现思路是通过某些手段给目标应用注入追踪探针（Probe）。探针在结构上可视为一个寄生在目标服务身上的小型微服务系统，它一般会有自己专用的服务注册、心跳检测功能有专门的数据收集协议，把从目标系统中监控到的服务调用信息，通过另外一次独立的HTTP或者RPC请求发送给追踪系统（llj：库打桩或者装饰器模式，前后埋入代码）</li>
<li><strong>基于边车代理的追踪</strong>：Sidecar-Based Tracing。服务网格的专属方案，也是最理想的分布式追踪模型，它对应用完全透明，无论是日志还是服务本身都不会有任何变化；它与程序语言无关，无论应用采用什么变成语言实现，只要它还是通过网络（HTTP或gRPC）来访问服务就可以被追踪到；他有独立的数据通道，追踪数据通过控制平面上报，避免了追踪对程序通信或者日志收集的依赖和困扰，保证了最佳的精确性（代表产品：Envoy）</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>聚合度量</strong>：主流Prometheus</p>
<ul>
<li><p><strong>度量</strong>：Metrics。指对系统中某一类信息的统计聚合。度量是用经过聚合统计后的高维度新信息，以最简单只管的形式来总结复杂的过程，为监控、预警提供决策支持。<strong>度量的主要目的是监控和预警，揭示系统的总体运行状态</strong></p>
</li>
<li><p>度量总体上可分为</p>
<ul>
<li><strong>客户端的指标收集</strong><ul>
<li><strong>如何定义指标？</strong><ul>
<li><strong>计数度量器</strong></li>
<li><strong>瞬态度量器</strong></li>
<li><strong>吞吐率度量器</strong>：用于统计单位时间的吞吐量，即单位时间内某个事件的发生次数，如TPS</li>
<li><strong>直方图度量器</strong></li>
<li><strong>采样点分位图度量器</strong></li>
</ul>
</li>
<li><strong>如何将这些指标告诉服务端？</strong><ul>
<li><strong>拉取式采集</strong></li>
<li><strong>推送式采集</strong></li>
</ul>
</li>
</ul>
</li>
<li><strong>服务端的存储查询</strong><ul>
<li><strong>时序数据库</strong>：用于存储根据时间变化的数据，并且以时间来建立索引的数据库<ul>
<li>以日志结构的合并树代替传统关系型数据库中的B+树</li>
<li>设置激进的数据保留策略</li>
<li>对数据进行再采样（越久的日志按越长的时间跨度采样）</li>
</ul>
</li>
</ul>
</li>
<li><strong>终端的监控预警</strong><ul>
<li>Prometheus提供了专门用于预警的Alert Manager，将Alert Manager与Prometheus关联后，可以设置某个指标在多长时间内达到何种条件就会触发预警状态，触发预警后，根据路由中配置的接收器，譬如邮件接受器、Slack接收器、微信接收器，或者更通用的WebHook接收器等来自动通知用户</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="第四部分-不可变设施基础"><a href="#第四部分-不可变设施基础" class="headerlink" title="第四部分 不可变设施基础"></a>第四部分 不可变设施基础</h1><h2 id="第十一章-虚拟化容器"><a href="#第十一章-虚拟化容器" class="headerlink" title="第十一章 虚拟化容器"></a>第十一章 虚拟化容器</h2><ul>
<li><strong>容器的首要目标</strong>：让软件分发部署过程从传统的发布安装包、靠人工部署转变为直接发布已部署好的、包含整套运行环境的虚拟化镜像</li>
<li><strong>一个计算机软件要能够正确运行，需要三方面兼容性共同保障</strong><ul>
<li><strong>ISA兼容</strong>：目标机器指令集的兼容性。ARM架构机器无法运行面向x86架构编译的程序</li>
<li><strong>ABI兼容</strong>：目标系统或者依赖库的<strong>二进制兼容性</strong>。譬如Windows系统环境无法直接运行Linux的程序</li>
<li><strong>环境兼容</strong>：目标环境的兼容性。没有正确的配置文件、环境变量等无法运行</li>
</ul>
</li>
<li><strong>虚拟化技术</strong>：笔者把使用仿真（Emulation）以及虚拟化（Virtualization）技术来解决以上三项兼容性问题的方法都统称为虚拟化技术。根据抽象目标和兼容性高低的不同，虚拟化技术又分为五类<ul>
<li><strong>指令集虚拟化</strong>：ISA Level Virtualization。通过软件来模拟不同ISA架构的处理器的工作过程</li>
<li><strong>硬件抽象层虚拟化</strong>：Hardware Abstraction Level Virtualization。以软件或者直接通过硬件来模拟处理器、芯片组、内存、磁盘控制器、显卡等设备的工作过程。<strong>一般人们所说的“虚拟机”就是指这类虚拟化技术</strong><ul>
<li>使用纯软件的二进制翻译来模拟虚拟设备</li>
<li>将物理设备直通到虚拟机中使用，典型代表为VMware ESXi和Hyper-V</li>
</ul>
</li>
<li><strong>操作系统层虚拟化</strong>：OS Level Virtualization。无论是指令集虚拟化还是硬件抽象层虚拟化，都会运行一套完全真实的操作系统来解决ABI兼容性和环境兼容性问题。操作系统层虚拟化不会提供真实的操作系统，而是采用了隔离手段，使得不同进程拥有独立的系统资源和资源配额，看起来仿佛是独享了整个操作系统，但其实系统的内核仍然是被不同的进程所共享的。<strong>操作系统层虚拟化的另一个名字就是“容器化”（Containerization）</strong>，由此可见，容器化仅仅是虚拟化的一个子集，只能提供操作系统内核以上的部分ABI兼容性与完整的环境兼容性。<strong>容器化牺牲了一定的隔离性与兼容性，换来的是比前面两种虚拟化更高的启动速度、运行性能和更低的执行负担</strong></li>
<li><strong>运行库层的虚拟化</strong>：Library Level Virtualization。与操作系统层虚拟化采用隔离手段来模拟系统不同，运行库层虚拟化选择使用软件翻译的方法来模拟系统，它以一个独立的进程来代替操作系统内核，可提供目标软件运行所需的全部能力。典型的又WINE和WSL（llj：甚至没听说过这个概念）</li>
<li><strong>语言层虚拟化</strong>：Programming Language Level Virtualization。由虚拟机将高级语言生成的中间代码转换为目标机器可以直接执行的指令，如JVM</li>
</ul>
</li>
<li><strong>容器历史</strong><ul>
<li><strong>隔离文件：chroot</strong><ul>
<li><strong>chroot</strong>：当某个进程经过chroot操作后，它的根目录就会被锁定在命令参数指定的位置，以后它或者它的子进程将不能再访问和操作该目录之外的其他文件</li>
<li><strong>pivot_root</strong>：直接切换了根文件系统（rootfs），有效地避免了chroot命令可能出现的安全性漏洞。LXC、Docker等也都是优先使用pivot_root来是西安根文件系统切换的</li>
</ul>
</li>
<li><strong>隔离访问：名称空间</strong><ul>
<li><strong>Linux名称空间</strong>：一种内核直接提供的全局资源封装，是内核针对进程设计的访问隔离机制。进程在一个独立的Linux名称空间中朝系统看去，会觉得自己仿佛就是这方天地的主人，拥有这台Linux主机上的一切资源，不仅文件系统时独立的，还有着独立的PID编号、UID/GID编号、网络等等</li>
</ul>
</li>
<li><strong>隔离资源：cgroups</strong><ul>
<li>必须限定资源，否则其他进程可能会被挂起。Linux系统解决这个问题的方案时控制群组（Control Groups，cgroups）。直接由内核提供功能，用于隔离或者分配并限制某个进程组能够使用的资源配额。资源配额包括处理器时间、内存大小、磁盘I/O速度等</li>
</ul>
</li>
<li><strong>封装系统：LXC</strong>：<ul>
<li><strong>LinuX Container</strong>：为降低普通用户综合使用namespaces、cgroups这些低级特性的门槛。LXC严重的容器与OpenVZ和Linux-VServer定义的并无差别，是一种封装系统的轻量级虚拟机，而Docker眼中的容器是一种封装应用的技术手段</li>
<li>以封装系统为出发点，仍是按照先装系统再装软件的思路，就永远无法在一两分钟甚至十几秒就构建出一个合乎要求的软件运行环境，也决定了LXC不可能形成今天的容器生态</li>
</ul>
</li>
<li><strong>封装应用：Docker</strong><ul>
<li><strong>开放容器交互标准OCI</strong><ul>
<li><strong>运行时标准</strong>：runtime-spec。定义了应该如何运行一个容器、如何管理容器的状态和生命周期、如何使用操作系统底层特性（namespaces、cgroups、pivot_root等）</li>
<li><strong>容器镜像标准</strong>：image-spec。规定了容器镜像的格式、配置、元数据的格式，可以理解为镜像的静态描述</li>
<li><strong>镜像分发标准</strong>：distribution-spec。规定了镜像推送和拉取的网络交互过程</li>
</ul>
</li>
</ul>
</li>
<li><strong>封装集群：Kubernetes</strong><ul>
<li>如果说以Docker为代表的容器引擎是将软件的发布流程从分发二进制安装包转变为直接分发虚拟化后的整个运行环境，令应用得以实现跨机器的绿色部署，那以Kubernetes为代表的容器编排框架就是把大型软件系统运行所依赖的集群环境也进行了虚拟化，令集群得以实现跨数据中心的绿色部署，并能够根据实际情况自动扩展</li>
</ul>
</li>
</ul>
</li>
<li><strong>容器编排</strong>：Container Orchestration。单体时代过去之后，分布式系统里应用的概念已不再等同于进程，此时的应用需要多个进程共同协作，通过集群的形式对外提供服务，而以虚拟化方法实现这个目标的过程就被称为~。容器之间顺畅地交互通信时协作的<strong>核心需求</strong></li>
<li>Docker只能通过监视PID为1的进程（由ENTRYPOINT启动的进程）的运行状态来判断容器的工作状态是否正常然后根据状态决定是否执行清理自动重启等工作</li>
<li>容器的本质时对cgroups和namespaces所提供的隔离能力的一种封装，在Docker提倡的单进程封装的理念影响下，容器蕴含的隔离性多了仅针对单个进程的额外限制，而Linux的cgroups和namespaces原本都是针对进程组而非单个进程来设计的，同一个进程组中的多个进程天然就可以共享相同的访问权限了资源配额。如果我们将容器和进程在概念上对应起来，那容器编排的第一个扩展点，就是找到与进程组对应的概念，在Kubernetes中，叫作Pod（Production Task，生产任务）<ul>
<li>容器 &lt;-&gt; 进程</li>
<li>Pod &lt;-&gt; 进程组</li>
</ul>
</li>
<li><strong>Pod两大基本职责</strong><ul>
<li><strong>扮演容器组的角色，满足容器共享名称空间的需求</strong>。同一个Pod内的多个容器相互之间以超亲密的方式协作，共享以下内容（<strong>只有PID名称空间和文件名称空间默认是隔离的</strong>）<ul>
<li><strong>UTS名称空间</strong>：所有容器都有相同的主机名和域名</li>
<li><strong>网络名称空间</strong>：共享一样的网卡、网络栈、IP地址等</li>
<li><strong>IPC名称空间</strong>：所有容器都可以通过信号量或者POSIX共享内存等方式通信</li>
<li><strong>时间名称空间</strong>：共享系统时间</li>
</ul>
</li>
<li><strong>实现原子性调度</strong>：如果容器编排不跨越集群节点，是否具有原子性都无关紧要。但是在集群环境中，在容器可能跨机器调度时，这个特性就变得非常重要。如果以容器为单位来调度，不同的容器就有可能被分配到不同的机器上。两台机器之间本来就是物理隔离，依靠网络连接的，这时候谈什么名称空间共享、cgroups配额共享都将毫无意义<ul>
<li><strong>协同调度</strong>：Coscheduling。保证一组紧密联系的任务能够被同时分配资源</li>
</ul>
</li>
</ul>
</li>
<li>Pod内部多个容器共享UTS、IPC、网络等名称空间是通过一个名为Infra Container的容器来实现的，这个容器时整个Pod中第一个启动的容器，Pod中其他容器都会以Infra Container作为父容器，UTS、IPC、网络名称空间事实上都来自Infra Container</li>
<li>Pod是隔离与调度的基本单位。Kubernetes将一切视为资源，不同资源依靠层级关系相互组合、协作的这个思想是贯穿Kubernetes整个系统的两大核心设计理念之一</li>
<li><strong>Kubernetes某些计算资源的设计意图</strong><ul>
<li><strong>容器</strong>：镜像管理的最小单位</li>
<li><strong>生产任务Pod</strong>：资源调度的最小单位。补充了容器化后缺失的与进程组对应的“容器组”的概念</li>
<li><strong>节点Node</strong>：硬件单元的最小单位。是处理器和内存等资源的资源池</li>
<li><strong>集群Cluster</strong>：集群是处理元数据的最小单位</li>
<li><strong>集群联邦Federation</strong>：对应多个集群。常见应用是能满足可用区域多活、跨地域容灾的需求</li>
</ul>
</li>
<li><strong>控制器模式</strong>：继资源模型之后，另一个Kubernetes核心设计理念，可以实现具有韧性和弹性的系统<ul>
<li><strong>控制回路</strong>：Control Loop。让编排系统在这些服务出现问题或者运行状态不正确的时候，能自动调整成正确的状态</li>
<li><strong>Kubernetes声明式API</strong>：通过描述清楚这些资源的期望状态，由Kubernetes种对应监视这些资源的控制器来驱动资源的实际状态逐渐向期望状态靠拢，这种交互风格被称为~</li>
<li>与资源相对应，只要是实际状态有可能发生变化的资源对象，通常都会由对应的控制器进行追踪，每个控制器至少会追踪一种类型的资源对象。未来管理众多资源控制器，Kubernetes设计了统一的控制器管理框架（kube-controller-manager）来维护这些控制器的正常运作，以及统一的指标监视器（kube-apiserver）来为控制器提供其工作时追踪资源的度量数据</li>
</ul>
</li>
<li><strong>滚动更新</strong>：Rolling Update。指先停止少量旧副本，维持大量旧副本继续提供服务，当停止的旧副本更新成功，新副本可以提供服务以后，再重复以上操作，直至所有副本都更新成功</li>
<li><strong>故障恢复、滚动更新、自动扩缩这些特性，在云原生时代里常被概括成服务的韧性与弹性（Resilience、Elasticity）</strong></li>
<li><strong>资源和控制器是贯穿整个Kubernetes的两大设计理念</strong></li>
<li>Kubernetes被誉为<strong>云原生时代的操作系统</strong></li>
<li>云原生基础设施的其中一个重要目标是接管业务系统复杂的非功能特性，让业务研发与运维工作变得足够简单，不受分布式的牵绊，然而Kubernetes被诟病最多的就是复杂，自诞生之日起就以陡峭的学习曲线</li>
<li><strong>Kustomize</strong>：主要价值是根据环境来生成不同的部署配置</li>
<li><strong>Helm</strong>：如果说Kubernetes是云原生操作系统，那Helm就要成为这个操作系统上的应用商店与包管理工具。提出了与Linux包管理直接对应的Chart格式和Repository应用仓库</li>
<li><strong>Operator</strong>：使用自定义资源，管理应用及其组件的自定义Kubernetes控制器</li>
<li><strong>状态</strong>：<ul>
<li><strong>有状态应用</strong>：Stateful Application。程序推倒重来后，用户能察觉到该应用已经发生变化<ul>
<li><strong>StatefulSet和StatefulSetController</strong><ul>
<li>Pod会按顺序创建和销毁</li>
<li>Pod具有稳定的网络名称：ReplicaSet中名字随机产生，而StatefulSet用顺序的编号且重启后依然不变</li>
<li>Pod具有稳定的持久化存储：每个Pod都可以拥有自己独立的PersistentVolumeClaim资源，即使Pod被重新调度到其他节点上，它所拥有的磁盘也依然会被挂在到该Pod</li>
</ul>
</li>
</ul>
</li>
<li><strong>无状态应用</strong>：Stateless Application。程序每次运行都跟首次运行一样，不会依赖任何操作遗留下的痕迹（llj：不如叫幂等应用）</li>
</ul>
</li>
<li><strong>应该如何封装应用才是最佳的实践，目前尚且没有定论，但是以应用为中心的理念却已经成为明确的共识</strong></li>
</ul>
<h2 id="第十二章-容器间网络"><a href="#第十二章-容器间网络" class="headerlink" title="第十二章 容器间网络"></a>第十二章 容器间网络</h2><ul>
<li>如何基于Linux系统的网络虚拟化技术来实现容器间网络通信</li>
<li><strong>Netfilter</strong>：Linux Kernel 2.4版本开始，内核开放的一套通用的可供代码干预数据在协议栈中流转的过滤器框架，它围绕网络层（IP协议）的周围，埋下了<strong>五个钩子</strong>，每当有数据包流到网络层，经过这些钩子时，就会自动触发由内核模块注册在这里的回调函数<ul>
<li><strong>五个钩子</strong>：<ul>
<li><strong>PREROUTING</strong>：来自设备的数据包进入协议栈后立即触发此钩子</li>
<li><strong>INPUT</strong>：报文经过IP路由后，如果确定时发往本机的，将会触发此钩子</li>
<li><strong>FORWAED</strong>：报文经过IP路由后，如果确定不是发往本机的，将会触发此钩子</li>
<li><strong>OUTPUT</strong>：从本机程序发出的数据包，在经过IP路由前，将会触发此钩子</li>
<li><strong>POSTROUTING</strong>：从本机网卡发出的数据包，都会触发此钩子</li>
</ul>
</li>
<li>Netfilter允许在同一个钩子处注册多个回调函数，因此向钩子注册回调函数时必须提供明确的优先级，钩子触发的回调函数集合被称为<strong>回调链（Chained Callback）</strong></li>
<li>以Netfilter为基础的应用，使用最广泛的要数Xtables系列工具，如iptables。iptables比较贴切的定位应是能够代替Netfilter多数常规功能的IP包过滤工具</li>
</ul>
</li>
<li><strong>iptables</strong>：内置了五张不可扩展的规则表，优先级从上至下。Kubernetes用来管理Services的Endpoints的核心组件kube-proxy就依赖iptables来完成ClusterIP到Pod的通信（也可以采用IPVS，IPVS同样是基于Netfilter的）<ul>
<li>raw表：用于去除数据包上的连接追踪机制</li>
<li>mangle表：用于修改数据包的报文头信息，如服务类型、生存周期以及为数据包设置Mark标记</li>
<li>nat表：用于修改数据包的源或者目的地址等信息，典型应用是网络地址转换</li>
<li>filter表：用于对数据包进行过滤，控制到达某条链上的数据包时继续放行、直接丢弃或拒绝，典型应用是防火墙</li>
<li>security表：用于在数据包上应用SELinux，不常用</li>
</ul>
</li>
<li><strong>目前主流的虚拟网卡方案</strong><ul>
<li><strong>tun/tap</strong>：一组通用的虚拟驱动程序包，包含<strong>两个设备</strong><ul>
<li>两个设备<ul>
<li><strong>虚拟网卡驱动</strong>：用于网络数据包处理</li>
<li><strong>字符设备</strong>：用于内核空间与用户空间交互</li>
</ul>
</li>
<li>tun和tap是两个相对独立的虚拟网络设备<ul>
<li><strong>tap</strong>：模拟了以太网设备，操作二层数据包（以太帧）</li>
<li><strong>tun</strong>：模拟了网络层设备，操作三层数据包（IP报文）</li>
</ul>
</li>
</ul>
</li>
<li><strong>veth</strong>：Linux Kernel 2.6版本，Linux在开始支持网络名称空间隔离的同时，也提供了专门的虚拟以太网Virtual Ethernet（veth）。<ul>
<li>要使用veth，必须在两个独立的网络名称空间中进行才有意义</li>
<li>veth以模拟网卡直连的方式很好地解决了两个容器之间地通信问题</li>
</ul>
</li>
</ul>
</li>
<li><strong>隧道Tunneling</strong>：将一个数据包套进另一个数据包的处理方式</li>
<li><strong>Linux Bridge</strong>：Linux Kernel 2.2开始提供地二层转发工具，由brctl命令创建和管理</li>
<li><strong>SDN</strong>：软件定义网络的核心思路是在物理网路上再构造一层虚拟化网络，将控制平面和数据平面分离开来，实现流量的灵活控制，为核心网络及应用的创新提供良好的平台<ul>
<li><strong>Underlay</strong>：SDN里位于下层的额物理网络，着重解决网络的连通性和可管理性</li>
<li><strong>Overlay</strong>：位于上层的逻辑网络，着重为应用提供与软件需求相符的传输服务和网络拓扑</li>
</ul>
</li>
<li><strong>VLAN</strong>：Virtual Local Area Network。首要职责是划分广播域，将连接在同一个物理网络上的设备区分开来，划分的具体方法是在以太帧的报文头中加入<strong>VLAN Tag</strong>，让所有广播指针对具有相同VLAN Tag的设备生效。这样既缩小了广播域，也提高了安全性和可管理性，因为两个VLAN之间不能直接通信。如果确实有需要，就必须通过三层设备来进行，譬如单臂路由或者三层交换机</li>
<li><strong>VXLAN</strong>：典型的Overlay网络。<ul>
<li>VXLAN采用L2 over L4（MAC in UDP）的报文封装模式，把原本在二层传输的以太帧放到四层UDP协议的报文体内，同时加入自己定义的VXLAN Header。VXLAN Header里直接就有24位的VLAN ID，同样可以存储1677万个不同取值，使得二层网络可以在三层范围内进行扩展</li>
<li>只要三层可达的网络就能部署VXLAN。VXLAN的每个边缘入口上都布置了一个VTEP（VXLAN Tunnel Endpoint）设备，它既可以是物理设备，也可以是虚拟设备，负责VXLAN协议报文的封包和解包</li>
</ul>
</li>
<li><strong>MACVLAN</strong>：借用了VXLAN子接口的思路，并且在这个基础上进一步优化，不仅允许为同一个网卡设置多个IP地址，还允许在同一张网卡上设置多个MAC地址。由同一个物理网卡虚拟出来的副本网卡，天然处于同一个VLAN中，可以直接二层通信，不需要将流量转发到外部网络</li>
<li>所有的容器网络通信问题，都可以归结为本地主机内部的多个容器之间、本地主机与内部容器之间和跨越不同主机的多个容器之间的通信问题</li>
<li>安装Docker过程中会自动在宿主机上创建一个名为docker0的网桥，以及三种不同Docker网络，分别是bridge、host、none，这三种网络，对应Docker提供的三种开箱即用的网络方案<ul>
<li><strong>桥接模式</strong>：使用–network=bridge指定，这也是未指定网络参数时的默认网络。桥接模式下，Docker会为新容器分配独立的网络名称空间，创建好veth pair，一端接入容器，另一端接入docker0网桥。Docker会为每个容器自动分配好IP地址，默认配置下的地址范是172.17.0.0/24，docker0的默认地址是172.17.0.1，并且设置所有的容器网关为docker0，这样所有接入同一个网桥内的容器可以直接依靠二层网络来通信，而在此范围之外的容器、主机就必须通过网关来访问</li>
<li><strong>主机模式</strong>：使用–network=host指定。主机模式下，Docker不会为新容器创建独立的网络名称空间，这样容器的一切网络设施，如网卡、网络栈等都直接使用宿主机上的真实设施，容器也就不会拥有自己的独立的IP地址</li>
<li><strong>空置模式</strong>：使用–network=none指定。空置模式下，Docker会给新容器创建独立的网络名称空间，但是不会创建任何虚拟的网络设备，此时容器能看到的只有一个回环设备而已</li>
<li><strong>容器模式</strong>：创建容器后使用–network=container:容器名称指定。容器模式下，新创建的容器将会加入指定的容器网络名称空间，共享一切网络资源。两个容器可以直接使用回环地址通信，但端口号等网络资源不能有冲突</li>
<li><strong>MACVLAN模式</strong>：使用docker network create -d macvlan来创建。此网络允许为容器指定一个副本网卡，容器通过副本网卡的MAC地址来使用宿主机上的物理设备</li>
<li><strong>Overlay模式</strong>：使用docker network create -d overlay创建。Docker所说的Overlay网络实际上就是特指VXLAN，这种网络模式主要用于Docker Swarm服务之间，Docker Swarm惜败Kubernetes，故没人用</li>
</ul>
</li>
<li><strong>CNI</strong>：容器网络接口<ul>
<li>提出容器网络标准的目的就是把网络功能从容器运行时引擎或者容器编排系统中剥离出去</li>
<li>功能<ul>
<li><strong>管理网络的创建与删除</strong></li>
<li><strong>管理IP地址分配与回收</strong></li>
</ul>
</li>
<li><strong>CNI跨主机通信的网络实现模式</strong><ul>
<li><strong>Overlay模式</strong>：虚拟化的上层逻辑网络，好处在于它不受底层物理网络结构的约束，有更大的自由度，更好的易用性；坏处是由于额外的包头封装导致信息密度降低，额外的隧道封包、解包会导致传输性能下降。只能通过三层转发这类被限制的网络的环境里，基本上只能选择Overlay网络插件<ul>
<li>常见的Overlay网络插件有Fannel（VXLAN模式）、Calico（IPIP模式）、Weave等</li>
</ul>
</li>
<li><strong>路由模式</strong>：属于Underlay的一种特例。相比于Overlay网络，路由模式的主要区别在于他的跨主机通信是直接通过路由转发来实现的，因而无需再不同主机之间进行隧道封包。路由模式依赖Linux内置在系统之中的路由协议，将路由表分发到子网的每一台物理主机。这样，当跨主机访问容器时，Linux主机可以根据自己的路由表得知该容器具体位于哪台物理主机之中，从而直接将数据包转发过去<ul>
<li>常见的路由网络有Fannel（HostGateway模式）、Calico（BGP模式）等</li>
</ul>
</li>
<li><strong>Underlay模式</strong>：特指让容器和宿主机处于同一个网络，两者拥有相同地位的网络方案。Underlay网络要求容器的网络接口能够直接与底层网络进行通信，因此该模式是直接依赖于虚拟化设备与底层网络能力的<ul>
<li>常见的Underlay网络插件有MACVLAN、SR-IOV等</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="第十三章-持久化存储"><a href="#第十三章-持久化存储" class="headerlink" title="第十三章 持久化存储"></a>第十三章 持久化存储</h2><ul>
<li>容器中的数据修改大多是基于写时复制策略来实现的，容器会利用叠加式文件系统（OverlayFS）的特性，在用户意图修改镜像时，自动将变更的内容写入独立区域，再与原有数据叠加到一起，使其从外观上看来像是“覆盖”了原有内容</li>
<li>Kubernetes将Volume分为两类<ul>
<li><strong>普通Volume</strong>：设计目标不是为了持久化地保存数据，而是为同一个Pod中多个容器提供可共享的存储资源，因此Volume具有十分明确的生存周期——与挂载它的Pod相同的生命周期</li>
<li><strong>Persistent Volume</strong>：PersistentVolume是由管理员负责提供的集群存储，PersistentVolumeClaim是由用户负责提供的存储需求<ul>
<li>Kubernetes对PersistentVolumeClaim与PersistentVolume的撮合结果是产生一对一的绑定关系</li>
</ul>
</li>
</ul>
</li>
<li><strong>动态存储分配方案</strong>：指在用户声明存储能力需求时，不是通过Kubernetes撮合来获得一个管理员人工预置的PersistentVolume，而是由特定的资源分配器（Provisioner）自动地在存储资源池或者云存储系统中分配符合用户存储需求地PersistentVolume，然后挂载到Pod中使用。完成这项工作地资源被命名为<strong>StorageClass</strong>，它的具体工作：<ul>
<li>管理员根据存储系统地实际情况，先准备好对应地资源分配器</li>
<li>管理员不再手工分配PersistentVolume，而是根据存储配置StorageClass</li>
<li>用户依然通过PersistentVolumeClaim来声明所需存储，但是应在声明中明确指出该有哪个StorageClass来代替Kubernetes处理该请求</li>
<li>撮合分配</li>
</ul>
</li>
<li>只描述意图而不关心中间具体的处理过程是声明式编程的精髓，也是流程自动化的必要基础</li>
<li>Kubernetes参考了传统操作系统接入或移除新存储设备的做法，把接入或移除外部存储分解为以下三种操作<ul>
<li>首先，决定应准备（Provision）哪种存储设备，Provision可类比为给操作系统扩容而购买了新的存储设备</li>
<li>然后，将准备好的存储设备附加（Attach）到系统中，Attach可类比为将存储设备接入到操作系统，此时尽管设备还不能使用，但已经可以用fdisk -l命令看到设备。这步确定了存储的设备名称、驱动方式等面向系统侧的信息，它的逆操作时分离（Detach）存储设备</li>
<li>最后，将附加好的存储挂载（Mount）到系统中，这步确定了存储设备的访问目录、文件系统格式等面向应用侧的信息，它的逆操作是卸载（Unmount）存储设备</li>
</ul>
</li>
<li>Kubernetes目前同时支持FlexVolume与CSI两套独立的存储扩展机制</li>
<li><strong>存储系统划分</strong>：根据存储系统提供何种形式的接口供外部访问来划分<ul>
<li><strong>块存储</strong>：<ul>
<li>块存储是数据存储最古老的形式，数据都存储在固定长度的一个或多个块中，想要读写访问数据，就必须使用与存储类型相匹配的协议（SCSI、SATA、SAS、FCP、iSCSI等）来进行。</li>
<li>硬盘通常会以多个块（这些块甚至可以来自不同的物理设备，譬如磁盘阵列）来组成一个逻辑分区，将分区进行高级格式化后就形成了卷（Volume）</li>
<li>块存储具有排他性，一旦块设备被某个客户端挂载就无法在访问上面的数据了，因此，Kubernetes中改在的块存储的访问模式大都要求必须是RWO（ReadWriteOnce）的</li>
</ul>
</li>
<li><strong>文件存储</strong>：</li>
<li><strong>对象存储</strong>：<ul>
<li>对象存储是相对较新的数据存储形式，是一种随着云数据中心的兴起而发展起来的存储，是以非结构化数据为目标的存储方案。这里的“对象”可以理解为一个元数据及与其配对的一个逻辑数据块的组合，元数据提供了对象所包含的上下文信息，譬如数据的类型、大小、权限、创建人、创建时间等，数据块则存储了对象的具体内容，也可以理解为数据和元数据共同构成了一个对象。</li>
<li>由于对象存储天生的分布式特性，以及极其低廉的扩展成本，很适合CDN一类的应用，用于存放图片和音视频等媒体内容以及网页和脚本等静态资源</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="第十四章-资源与调度"><a href="#第十四章-资源与调度" class="headerlink" title="第十四章 资源与调度"></a>第十四章 资源与调度</h2><ul>
<li><strong>资源</strong>：广义上讲，Kubernetes系统中所有能够接触的方方面面都被抽象成了资源，譬如表示工作负荷的资源，表示存储的资源，表示策略的资源，表示身份的资源。“一切皆资源”的设计是Kubernetes能够顺利施行声明式API的必要前提。Kubernetes以资源为载体，建立了一套同时囊括抽象元素（如策略、依赖、权限）和物理元素（如软件、硬件、网络）的领域特定语言。通过不同层级间资源的使用关系来描述上至整个集群甚至集群联邦，下至某一块内存区域或者一小部分处理器核心的状态<ul>
<li><strong>可压缩资源</strong>：Compressible Resource。特点是当可压缩资源不足时，Pod只会处于“饥饿状态”，运行变慢，但不会被系统杀死</li>
<li><strong>不可压缩资源</strong>：Incompressible Resource。特点是不可压缩资源不足，或者超过了容器自己声明的最大限度时，Pod就会因为内存溢出而被系统直接杀掉</li>
</ul>
</li>
<li><strong>调度</strong>：从编排系统的角度来看，Node是资源的提供者，Pod是资源的拥有者，调度是对两者进行恰当的撮合</li>
<li>Pod由一个到多个容器组成，资源最终交由Pod的各个容器去使用，所以资源的需求是设定在容器上的，具体配置是Pod的spec.containers[].resource.limits/requests.cpu/memory字段。针对Pod整体，Pod的资源配额无需手动设置，它就是其包含的每个容器资源需求的累加值</li>
<li>Kubernetes给出的配置中有requests和limits两个设置项（根据实际经验，大多数的工作负载在运行过程中真正使用到的资源，其实都远小于它所请求的资源）<ul>
<li><strong>requests</strong>是供调度器使用的，Kubernets选择哪个节点运行Pod，只会根据requests的值来进行决策</li>
<li><strong>limits</strong>才是供cgroups使用的，Kubernetes在向cgroups传递资源配额时，会按照limits的值来进行设置</li>
</ul>
</li>
<li><strong>服务质量等级</strong>：<ul>
<li><strong>Guaranteed</strong>：limits == requests</li>
<li><strong>Burstable</strong>：requests &lt; limits || 设置requests未设置limits</li>
<li><strong>BestEffort</strong>：requests和limits都未设置</li>
</ul>
</li>
<li><strong>优先级</strong>：除了服务质量等级外，Kubernetes还允许系统管理员自行决定Pod的优先级，这是通过类型PriorityClass的资源来实现的。优先级决定了Pod之间并不是平等的关系，且这种不平等不是谁会占用更多资源的问题，而是会直接影响Pod调度和生存的关键</li>
<li><strong>驱逐机制</strong>：Eviction。Pod的驱逐机制是通过kubelet来执行的，kubelet是部署在每个节点的集群管理程序，由于本身就运行在节点中，所以最容易感知到节点上较低服务质量等级的Pod，以保证其他更重要的Pod的安全。被驱逐的Pod中的所有容器都会被终止，Pod的状态也会变为Failed<ul>
<li>分类：<ul>
<li><strong>软驱逐</strong>：通常配置一个较低的警戒线，触及此线时，新系统将进入一段观察期。如果只是暂时的资源抖动，在观察期内能恢复到正常水平话，那就不会真正启动驱逐操作。否则，就会触发Pod的优雅退出（Grace Shutdown），系统会通知Pod让其自行了断，优雅退出期结束后，系统会强制杀掉还未曾自行了断的Pod</li>
<li><strong>硬驱逐</strong>：通常配置一个较高的警戒线，一旦触及此红线，立即强制杀掉Pod，而不会优雅退出。</li>
</ul>
</li>
<li><strong>软驱逐是为了减少资源抖动对服务的影响，硬驱逐是为了保障核心系统的稳定</strong></li>
<li>Kubernetes提供了–eviction-minimum-reclaim参数设置一旦驱逐发生后，至少清理出来多少资源才会终止</li>
<li>Kubernetes还提供了另一个参数–eviction-pressure-transition-period来约束调度器，设置在驱逐发生之后多长时间内不得网该节点调度Pod</li>
<li>Kubernetes还没成熟到变为“傻瓜式”容器编排系统之前，因地制宜地配置和运维是非常必要的</li>
</ul>
</li>
<li><strong>Predicate筛选算法和Priority评价算法</strong></li>
<li>Kubernetes中默认有三种过滤策略<ul>
<li><strong>通用过滤策略</strong>：最基础的调度过滤策略，用来检查节点能否满足Pod声明中需要的资源需求</li>
<li><strong>卷过滤策略</strong>：与存储相关的过滤策略，用来检查节点挂载的Volume是否存在冲突，或者Volume的可用区域是否与目标节点冲突</li>
<li><strong>节点过滤策略</strong>：与宿主机相关的过滤策略，最典型的是Kubernetes的污点与容忍度机制（Taint and Toleration）</li>
</ul>
</li>
<li>调度器并不会直接与kubelet通信来创建Pod，它只需要把待调度的Pod的nodeName字段更行为目标节点的名字即可，kubelet本身会监视该值的变化来接手后续工作</li>
</ul>
<h2 id="第十五章-服务网格"><a href="#第十五章-服务网格" class="headerlink" title="第十五章 服务网格"></a>第十五章 服务网格</h2><p>llj：不是很恰当的比喻，服务网格就是将以电线连接电子元件方式变成了印刷电路板来连接电子元件</p>
<ul>
<li>服务网格之所以能够获得企业与社区的重视，是因为它很好地弥补了编排系统对分布式应用粒度管控能力不足的缺憾</li>
<li>服务网格是一种处理程序间通信的基础设施，典型的存在形式是部署在应用旁边，一对一为应用提供服务的边车代理以及管理这些边车代理的控制程序</li>
<li>服务网格无需应用程序的任何配合，就能强制性地对应用通信进行管理。它使用了类似网络攻击的中间人流量劫持的手段，完全透明地接管容器与外界地通信，将管理地粒度从容器级别细化到每个单独地远程服务级别，使得基础设施干涉应用程序、介入程序行为的能力大为增强。从容器粒度延申到远程访问，分布式系统继容器和容器编排之后，又发掘到另一块更广袤的舞台空间</li>
<li><strong>分布式服务的通信演化</strong><ul>
<li><strong>第一阶段</strong>：将通信的非功能性需求视作业务需求的一部分，通信的可靠性由开发人员来保障</li>
<li><strong>第二阶段</strong>：将代码中的通信功能抽离重构成公共组件库，通信的可靠性由专业的平台开发人员来保障<ul>
<li>目前，基于公共组件库开发微服务仍然是应用<strong>最为广泛的解决方案</strong>，却不是一种完美的解决方案，这是微服务基础设施完全成熟之前必然会出现的应用形态，同时也一定是微服务进化过程中必然会被替代的过渡形态</li>
</ul>
</li>
<li><strong>第三阶段</strong>：将负责通信的公共组件库分离到进程之外，程序间通过网络代理来交互，通信的可靠性由专门的网络代理提供商来保障</li>
<li><strong>第四阶段</strong>：将网络代理以边车形式注入到应用容器，自动劫持应用的网络流量，通信的可靠性由专门的通信基础设施来保障<ul>
<li>目前边车代理的代表产品由Linkerd、Envoy、MOSN等</li>
</ul>
</li>
<li><strong>第五阶段</strong>：将边车代理统一管控卡里实现安全、可控、可观测的通信，将数据平面与控制平面分离开来，实现通用、透明的通信，这项工作由专门的服务网格框架来保障<ul>
<li>两类流量<ul>
<li><strong>控制平面</strong>：Control Plane。代理与管理器之间传递控制信息</li>
<li><strong>数据平面</strong>：Data Plane。代理与代理之间转发程序的数据包</li>
</ul>
</li>
<li>软件定义网络将解耦的数据平面与控制平面作为其最主要特征之一</li>
<li>分离数据平面和控制平面的实质是将“程序”和“网络”进行解耦，将网络可能出现的问题（中断后重试、降级等），与可能需要的功能（譬如实现追踪度量）的处理过程从程序中拿出，放到由控制平面指导的数据平面通信中去处理，制造处一种“这些问题在通信中根本不存在”的假象，仿佛网络和远程服务都是完美可靠的</li>
</ul>
</li>
</ul>
</li>
<li><strong>数据平面</strong>：数据平面由一系列边车代理构成，核心职责是转发应用的入站（Inbound）和出站（Outbound）数据包，因此也被称为转发平面（Forwarding Plane）。数据平面必须根据控制平面下发策略的指导，在应用无感知的情况下自动完成服务路由、健康检查、负载均衡、认证鉴权、产生监控数据等一系列工作，为完成这些工作至少需要解决三个问题<ul>
<li><strong>代理注入</strong><ul>
<li><strong>基座模式</strong>：这种方式接入的边车代理是不透明的，它至少会包括一个轻量级的SDK，通信由SDK中的接口去处理</li>
<li><strong>注入模式</strong>：<ul>
<li><strong>手动注入模式</strong>：对使用者不透明，对程序透明。在Kubernetes中要进行手动注入十分简单，只需要为Pod增加一个容器而已</li>
<li><strong>自动注入模式</strong>：对使用者和程序都透明。Istio推荐的代理注入方式。Kubernetes中，服务网格一般是依靠“动态准入控制”（Dynamic Admission Control）中的Mutating Webhook控制器来实现自动注入的</li>
</ul>
</li>
</ul>
</li>
<li><strong>流量劫持</strong>：边车代理做流量劫持最典型的方式是基于iptables进行的数据转发。Istio注入边车代理后，处了生成封装Envoy的istio-proxy容器外，还会生成一个initContainer，作用是自动修改容器的iptables<ul>
<li>用iptables进行流量劫持是最经典、最通用的手段</li>
<li>目前，如何实现更优化的数据平面流量劫持，是服务网格发展的前沿研究课题之一（可行的优化方案是使用eBPF，在Socket层面直接转发，不需要经过下面TCP/IP协议栈处理）</li>
</ul>
</li>
<li><strong>可靠通信</strong>：Envoy将代理转发行为抽象成三种资源，又定义了一些列访问这些资源的API<ul>
<li><strong>Listener</strong>：可以简单理解为Envoy的一个监听端口，用于接受来自下游应用程序的数据</li>
<li><strong>Cluster</strong>：Cluster是Envoy能够连接到的一组逻辑上提供相同服务的上游主机</li>
<li><strong>Router</strong>：Listener负责接受来自下游的数据，Cluster负责将数据转发给上游的服务，而Router则决定Listener在接收到下游的数据后，具体该如何将数据交给哪一个Cluster处理</li>
</ul>
</li>
</ul>
</li>
<li><strong>控制平面</strong>：控制平面的特点是不直接参与程序间通信，只会与数据平面中的代理通信，在程序不可见的背后，默默地完成下发配置和策略，指导数据平面工作<ul>
<li>1.5版本后，Istio重新回到单体架构。单体化之后出现的新进程istiod就承担了所有控制平面的职责，包括<ul>
<li><strong>数据平面交互</strong><ul>
<li><strong>边车注入</strong>：在Kubernetes中注册Mutating Webhook控制器，实现代理的自动注入，并生成Envoy的启动配置信息</li>
<li><strong>策略分发</strong>：为所有Envoy代理提供符合xDS协议策略分发服务</li>
<li><strong>配置分发</strong>：负责监听来自多种支持配置源的数据，譬如kube-apiserver，本地配置文件，或者定义为网格配置协议的配置信息</li>
</ul>
</li>
<li><strong>流量控制</strong>：这通常是用户使用服务网格的最主要目的<ul>
<li><strong>请求路由</strong>：通过VirtualService、DestinationRule等Kubernetes自定义资源实现了灵活的服务版本切分与规则路由，譬如以服务的迭代版本号作为路由规则来控制流量，实现诸如金丝雀发布这类应用需求</li>
<li><strong>流量治理</strong>：包括熔断、超时、重试等功能，譬如通过修改Envoy的最大连接数，实现对请求的流量控制；通过修改负载均衡策略，在轮询、随机、最少访问等方式间进行切换；通过设置异常探测策略，将满足异常条件的实力从负载均衡中摘除，以保证服务的稳定性等等</li>
<li><strong>调试能力</strong>：包括故障注入和流量镜像等功能</li>
</ul>
</li>
<li><strong>通信安全</strong>：包括通信中的加密、凭证、认证、授权等功能<ul>
<li><strong>生成CA证书</strong></li>
<li><strong>SDS服务代理</strong></li>
<li><strong>认证</strong>：提供基于节点的服务认证和基于请求的用户认证</li>
<li><strong>授权</strong>：提供不同级别的访问控制</li>
</ul>
</li>
<li><strong>可观测性</strong>：包括日志、追踪、度量三大能力<ul>
<li><strong>日志收集</strong>：不属于服务网格的处理范畴，通常使用ELK stack去完成</li>
<li><strong>链路追踪</strong>：为请求途径的所有服务生成分布式追踪数据并自动上报，运维人员可以通过Zipkin等追踪系统从数据中重建服务调用链</li>
<li><strong>指标度量</strong>：基于四类不同的监控标识（响应延迟、流量大小、错误数量、饱和度）生成一系列观测不同服务的监控指标，用于记录和展示网格中的服务状态</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><strong>服务网格的实质是数据平面产品与控制平面产品的集合</strong></li>
<li><strong>服务网格接口SMI</strong>：SMI规范包括四方面的API构成<ul>
<li><strong>流量规范</strong>：Traffic Specs。目标是定义流量的标识方式，譬如TCP流量、HTTP/1流量、HTTP/2流量、gRPC流量、WebSocket流量等该如何在配置中抽象及使用</li>
<li><strong>流量拆分</strong>：Traffic Split。目标是定义不同版本服务之间的流量比例，提供流量治理的能力，譬如限流、降级、容错等，以满足灰度发布、A/B测试等场景。<ul>
<li>SMI的流量拆分是直接基于Kubernetes的Service资源来设置的</li>
<li>Istio中则设计了VirtualService这样的新概念来解决相同的问题，通过Subset来拆分流量</li>
</ul>
</li>
<li><strong>流量度量</strong>：Traffic Metric。目标是为资源提供通用集成点，度量工具可以通过访问这些集成点来抓取指标。这部分完全遵循了Kubernetes的Metrics API进行扩充</li>
<li><strong>流量访问控制</strong>：Traffic Access Control。目标是根据客户端的身份配置，对特定的流量访问特定的服务提供简单的访问控制。<ul>
<li>SMI绑定了Kubernetes的ServiceAccount来做服务身份访问控制</li>
</ul>
</li>
</ul>
</li>
<li>P365有主流的服务网格技术</li>
</ul>
<h1 id="第五部分-技术方法论"><a href="#第五部分-技术方法论" class="headerlink" title="第五部分 技术方法论"></a>第五部分 技术方法论</h1><h2 id="第十六章-向微服务迈进"><a href="#第十六章-向微服务迈进" class="headerlink" title="第十六章 向微服务迈进"></a>第十六章 向微服务迈进</h2><ul>
<li>笔者个人的态度是旗帜鲜明地反对以“获得更好的性能”为主要目的，将系统重构为微服务架构，性能可能会作为辅助性地理由</li>
<li>硬件的成本能够持续稳定地下降，而软件开发地成本则不可能</li>
<li>选择微服务的一些常见因素<ul>
<li><strong>当意识到没有什么技术能够包打天下</strong><ul>
<li>很多时候为异构能力进行的分布式部署，不是你想不想的问题，而是没有选择</li>
</ul>
</li>
<li><strong>当个人能力因素成为系统发展的明显制约</strong></li>
<li><strong>当遇到来自外部商业层面对内部技术层面提出的要求</strong><ul>
<li>llj：产品是爹，爹说什么就是什么</li>
</ul>
</li>
<li><strong>变化发展特别快的创新业务系统往往会自主地向微服务架构靠近</strong></li>
<li><strong>大规模的、业务复杂的、历史包袱沉重的系统也可能主动向微服务架构靠近</strong></li>
</ul>
</li>
<li>微服务最主要的目的是对系统进行有效拆分，实现物理层面的隔离，微服务的核心价值就是拆分之后的系统能够让局部的单个服务有可能实现敏捷地卸载、部署、开发、升级，而局部地持续更迭，是系统整体具备Phoenix特性的必要条件</li>
<li>微服务的最终目的是构建出可持续的生态系统</li>
<li><strong>微服务需要的条件</strong><ul>
<li><strong>第一，决策者和执行者都能意识到康威定律在软件设计中的关键作用</strong><ul>
<li>不仅涉及技术问题，还要调整雇员组织结构，所以还是政治问题</li>
</ul>
</li>
<li><strong>第二，组织中具备一些对微服务有充分理解、有一定实践经验的技术专家</strong></li>
<li><strong>第三，系统应具有以自治为目标的自动化与监控度量能力</strong><ul>
<li><strong>微服务三个技术前提</strong><ul>
<li><strong>环境预置</strong>：Rapid Provisioning。即使不依赖云计算数据中心的支持，也有能力在短时间内迅速启动一台新的服务器</li>
<li><strong>基础监控</strong>：Basic Monitoring。监控体系由能力迅速捕捉到系统中出现的技术问题和业务问题</li>
<li><strong>快速部署</strong>：Rapid Deployment。有能力通过全自动化的部署管道，将服务的变更迅速部署到测试或生产环境中</li>
</ul>
</li>
</ul>
</li>
<li><strong>第四，复杂性已经成为制约生产力的主要矛盾</strong></li>
</ul>
</li>
<li><strong>粒度</strong><ul>
<li>微服务粒度的下界是它至少应满足独立（能够独立发布、独立部署、独立运行与独立测试），内聚（强相关的功能与数据在同一个服务中处理），完备（一个服务包含至少一项业务实体对应的完整操作）</li>
<li>微服务粒度的上界是一个2 Pizza Team能够在一个研发周期内完成的全部需求范围</li>
</ul>
</li>
<li><strong>治理</strong>：就是让产品能够符合预期地稳定运行，并能够持续保持在一定的质量水平上<ul>
<li>该定义将治理具体分解为“正确执行”和“持续保持”上</li>
<li>静态的治理<ul>
<li><strong>复杂性的来源</strong><ul>
<li><strong>认知负荷</strong>：Cognitive Load。在软件研发中表现为人接受业务、概念、模型、设计、接口、代码等信息所带来的负担大小系统中个体的认知负担越大，系统就越复杂</li>
<li><strong>协作成本</strong>：Collaboration Cost。在软件研发中表现为团队共同研发时付出的沟通、管理成本。系统个体间协作的成本越高，系统就越复杂</li>
</ul>
</li>
<li>软件规模小时微服务的复杂度高于单体系统，规模大时相反</li>
<li>微服务架构下，组织的拆分与产品的拆分对齐（康威定律），微服务系统的交互分为服务内部的进程内调用和服务之间的网络调用，组织的沟通也被拆分为团队内部的沟通和团队之间的协作，这种分治措施有利于控制沟通成本的增长速度，此时沟通成本的复杂度，就能缩减至经典分治算法的时间复杂度，即$O(NlogN)$</li>
<li>软件研发的整体复杂度是认知负荷与协作成本两者之和，对于单体架构是$O(K \times N) + O(N^2)$，对于微服务架构，整体复杂度就是$O(K \times N) + O(NlogN)$</li>
</ul>
</li>
<li>发展的治理<ul>
<li><strong>架构腐化</strong>：Architectural Decay。架构腐化只能延缓，无法避免<ul>
<li>架构腐化是软件动态发展中出现的问题，任何静态的治理方案都只能延缓，不能根治，必须在发展中才能找到彻底解决的办法。治理架构腐化唯一有效的办法是<strong>演进式的设计</strong></li>
</ul>
</li>
<li><strong>演进式设计</strong>：大型软件的建设是一个不断推倒重来的的演进过程，前一个版本对后一个版本的价值在于它满足了这个阶段用户的需要，让团队成员成功适应了这个阶段的复杂度，可以向下一个台阶迈进。<strong>不仅是建造的学问，也是破坏的学问</strong></li>
</ul>
</li>
</ul>
</li>
<li>现在，基于Kubernetes构筑的服务网格是目前最先进的架构风格，即通过中间人流量劫持的方式，以介于应用和基础设施之间的边车代理来做到既让用户代码可以专注业务需求，不必关注分布式的技术，又能实现几乎不亚于此前Spring Cloud时代的哪种通过代码来解决分布式问题的可配置、安全和可观测性</li>
</ul>

    </div>

    
    
    
        <div class="reward-container">
  <div>给我一块钱，可以不可以</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.jpg" alt="lilingj 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.jpg" alt="lilingj 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>


      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/02/24/%E7%90%86%E8%A7%A3fabirc%E7%9A%84channel%E5%92%8C%E5%8A%A8%E6%80%81%E6%B7%BB%E5%8A%A0/" rel="prev" title="理解channel和动态添加">
      <i class="fa fa-chevron-left"></i> 理解channel和动态添加
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86-%E6%BC%94%E8%BF%9B%E4%B8%AD%E7%9A%84%E6%9E%B6%E6%9E%84"><span class="nav-number">1.</span> <span class="nav-text">第一部分 演进中的架构</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E7%AB%A0-%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B%E5%8F%B2"><span class="nav-number">1.1.</span> <span class="nav-text">第一章 服务架构演进史</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86-%E6%9E%B6%E6%9E%84%E5%B8%88%E8%A7%86%E8%A7%92"><span class="nav-number">2.</span> <span class="nav-text">第二部分 架构师视角</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E8%AE%BF%E9%97%AE%E8%BF%9C%E7%A8%8B%E6%9C%8D%E5%8A%A1"><span class="nav-number">2.1.</span> <span class="nav-text">第二章 访问远程服务</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E4%B8%89%E7%AB%A0-%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86"><span class="nav-number">2.2.</span> <span class="nav-text">第三章 事务处理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E9%80%8F%E6%98%8E%E5%A4%9A%E7%BA%A7%E5%88%86%E6%B5%81%E7%B3%BB%E7%BB%9F"><span class="nav-number">2.3.</span> <span class="nav-text">第四章 透明多级分流系统</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E4%BA%94%E7%AB%A0-%E6%9E%B6%E6%9E%84%E5%AE%89%E5%85%A8%E6%80%A7"><span class="nav-number">2.4.</span> <span class="nav-text">第五章 架构安全性</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86-%E5%88%86%E5%B8%83%E5%BC%8F%E7%9A%84%E5%9F%BA%E7%9F%B3"><span class="nav-number">3.</span> <span class="nav-text">第三部分 分布式的基石</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E5%85%AD%E7%AB%A0-%E5%88%86%E5%B8%83%E5%BC%8F%E5%85%B1%E8%AF%86"><span class="nav-number">3.1.</span> <span class="nav-text">第六章 分布式共识</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E4%B8%83%E7%AB%A0-%E4%BB%8E%E7%B1%BB%E5%BA%93%E5%88%B0%E6%9C%8D%E5%8A%A1"><span class="nav-number">3.2.</span> <span class="nav-text">第七章 从类库到服务</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E5%85%AB%E7%AB%A0-%E6%B5%81%E9%87%8F%E6%B2%BB%E7%90%86"><span class="nav-number">3.3.</span> <span class="nav-text">第八章 流量治理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E4%B9%9D%E7%AB%A0-%E5%8F%AF%E9%9D%A0%E9%80%9A%E4%BF%A1"><span class="nav-number">3.4.</span> <span class="nav-text">第九章 可靠通信</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E5%8D%81%E7%AB%A0-%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7"><span class="nav-number">3.5.</span> <span class="nav-text">第十章 可观测性</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86-%E4%B8%8D%E5%8F%AF%E5%8F%98%E8%AE%BE%E6%96%BD%E5%9F%BA%E7%A1%80"><span class="nav-number">4.</span> <span class="nav-text">第四部分 不可变设施基础</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E5%8D%81%E4%B8%80%E7%AB%A0-%E8%99%9A%E6%8B%9F%E5%8C%96%E5%AE%B9%E5%99%A8"><span class="nav-number">4.1.</span> <span class="nav-text">第十一章 虚拟化容器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E5%8D%81%E4%BA%8C%E7%AB%A0-%E5%AE%B9%E5%99%A8%E9%97%B4%E7%BD%91%E7%BB%9C"><span class="nav-number">4.2.</span> <span class="nav-text">第十二章 容器间网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E5%8D%81%E4%B8%89%E7%AB%A0-%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8"><span class="nav-number">4.3.</span> <span class="nav-text">第十三章 持久化存储</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E5%8D%81%E5%9B%9B%E7%AB%A0-%E8%B5%84%E6%BA%90%E4%B8%8E%E8%B0%83%E5%BA%A6"><span class="nav-number">4.4.</span> <span class="nav-text">第十四章 资源与调度</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E5%8D%81%E4%BA%94%E7%AB%A0-%E6%9C%8D%E5%8A%A1%E7%BD%91%E6%A0%BC"><span class="nav-number">4.5.</span> <span class="nav-text">第十五章 服务网格</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86-%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95%E8%AE%BA"><span class="nav-number">5.</span> <span class="nav-text">第五部分 技术方法论</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E5%8D%81%E5%85%AD%E7%AB%A0-%E5%90%91%E5%BE%AE%E6%9C%8D%E5%8A%A1%E8%BF%88%E8%BF%9B"><span class="nav-number">5.1.</span> <span class="nav-text">第十六章 向微服务迈进</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="lilingj"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">lilingj</p>
  <div class="site-description" itemprop="description">昼短苦夜长，何不秉烛游？</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">18</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2020 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lilingj</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
